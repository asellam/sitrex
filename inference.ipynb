{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/asellam/sitrex.git\n",
        "%cd sitrex"
      ],
      "metadata": {
        "id": "DC-LL-GZA4ym",
        "outputId": "147b991b-6c1f-44c5-d53a-f730496b1678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DC-LL-GZA4ym",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sitrex'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 53 (delta 26), reused 34 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (53/53), 34.18 KiB | 3.80 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "/content/sitrex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "kCSGmSbYA-G5",
        "outputId": "d57d1320-d935-43f0-d848-2ac372bb426e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kCSGmSbYA-G5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12 (from -r requirements.txt (line 2))\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting numpy<2.0,>=1.23 (from -r requirements.txt (line 5))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m558.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.16.1)\n",
            "Requirement already satisfied: tqdm>=4.65 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.6.1)\n",
            "Requirement already satisfied: kagglehub>=0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.3.12)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.74.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (0.5.3)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (18.1.1)\n",
            "Collecting numpy<2.0,>=1.23 (from -r requirements.txt (line 5))\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (25.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (4.14.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (0.37.1)\n",
            "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting scipy>=1.10 (from -r requirements.txt (line 6))\n",
            "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 12)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 12)) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 15)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 15)) (2.32.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12->-r requirements.txt (line 2)) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2)) (0.5.3)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.7.0,>=0.7.0 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.7.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.6.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.2,>=0.6.2 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.6.2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.6.1,>=0.6.1 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.6.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2))\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (2025.8.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.3.1)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, scipy, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.3\n",
            "    Uninstalling wrapt-1.17.3:\n",
            "      Successfully uninstalled wrapt-1.17.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.3\n",
            "    Uninstalling jaxlib-0.5.3:\n",
            "      Successfully uninstalled jaxlib-0.5.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.3\n",
            "    Uninstalling jax-0.5.3:\n",
            "      Successfully uninstalled jax-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.19.0\n",
            "    Uninstalling tensorflow-2.19.0:\n",
            "      Successfully uninstalled tensorflow-2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download('hasyimabdillah/workoutfitness-video')"
      ],
      "metadata": {
        "id": "CD6rSFXQBDl_"
      },
      "id": "CD6rSFXQBDl_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218507cf-175c-483b-8500-a81b4ac3374b",
      "metadata": {
        "id": "218507cf-175c-483b-8500-a81b4ac3374b",
        "outputId": "f2db5780-f700-497b-b655-ca8c3f982b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "Siamese Model Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)           </span>┃<span style=\"font-weight: bold\"> Output Shape        </span>┃<span style=\"font-weight: bold\">     Param # </span>┃<span style=\"font-weight: bold\"> Connected to        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ input_layer_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                        │                     │             │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ transformer_block      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">74,944</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)     │                     │             │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ global_average_poolin… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling…</span> │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ global_average_poolin… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_block[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling…</span> │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ global_average_poo… │\n",
              "│                        │                     │             │ global_average_poo… │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ absolute_difference_l… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AbsoluteDifferenceLa…</span> │                     │             │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)          │         <span style=\"color: #00af00; text-decoration-color: #00af00\">759</span> │ absolute_differenc… │\n",
              "└────────────────────────┴─────────────────────┴─────────────┴─────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m    Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m23\u001b[0m)     │           \u001b[38;5;34m0\u001b[0m │ -                   │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)           │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ input_layer_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m23\u001b[0m)     │           \u001b[38;5;34m0\u001b[0m │ -                   │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)           │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m1,536\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                        │                     │             │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ transformer_block      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │      \u001b[38;5;34m74,944\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)     │                     │             │ dense[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ global_average_poolin… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m0\u001b[0m │ transformer_block[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling…\u001b[0m │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ global_average_poolin… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m0\u001b[0m │ transformer_block[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling…\u001b[0m │                     │             │                     │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │       \u001b[38;5;34m2,080\u001b[0m │ global_average_poo… │\n",
              "│                        │                     │             │ global_average_poo… │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ absolute_difference_l… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mAbsoluteDifferenceLa…\u001b[0m │                     │             │ dense_3[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├────────────────────────┼─────────────────────┼─────────────┼─────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)          │         \u001b[38;5;34m759\u001b[0m │ absolute_differenc… │\n",
              "└────────────────────────┴─────────────────────┴─────────────┴─────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,319</span> (309.84 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m79,319\u001b[0m (309.84 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,319</span> (309.84 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m79,319\u001b[0m (309.84 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usefulness Model Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                       </span>┃<span style=\"font-weight: bold\"> Output Shape               </span>┃<span style=\"font-weight: bold\">        Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ transformer_block_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">74,944</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                 │                            │                │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ global_average_pooling1d_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)           │                            │                │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>)                 │            <span style=\"color: #00af00; text-decoration-color: #00af00\">759</span> │\n",
              "└────────────────────────────────────┴────────────────────────────┴────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m, \u001b[38;5;34m23\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m1,536\u001b[0m │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ transformer_block_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │         \u001b[38;5;34m74,944\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                 │                            │                │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ global_average_pooling1d_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)           │                            │                │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                 │          \u001b[38;5;34m2,080\u001b[0m │\n",
              "├────────────────────────────────────┼────────────────────────────┼────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m)                 │            \u001b[38;5;34m759\u001b[0m │\n",
              "└────────────────────────────────────┴────────────────────────────┴────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,319</span> (309.84 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m79,319\u001b[0m (309.84 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">79,319</span> (309.84 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m79,319\u001b[0m (309.84 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "MODEL_SAVE_PATH_SIAMESE = './angle_similarity_model_L200.keras'\n",
        "MODEL_SAVE_PATH_CLASSIFIER = './angle_usefulness_model_L200.keras'\n",
        "\n",
        "# Load the trained Siamese model\n",
        "siamese_model = tf.keras.models.load_model(MODEL_SAVE_PATH_SIAMESE)\n",
        "siamese_model = tf.keras.models.load_model(MODEL_SAVE_PATH_CLASSIFIER)\n",
        "print(\"Models loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23778025-ea36-4eef-ad26-0c73b1091449",
      "metadata": {
        "id": "23778025-ea36-4eef-ad26-0c73b1091449"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def normalize(v):\n",
        "    norm = np.linalg.norm(v)\n",
        "    return v / norm if norm != 0 else v\n",
        "\n",
        "def point(landmark):\n",
        "    return np.array([landmark['x'], landmark['y'], landmark['z']])\n",
        "\n",
        "# Function to calculate the 3D angle between three points\n",
        "def angle_between(a, b):\n",
        "    cosine_angle = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-6)\n",
        "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
        "    return np.degrees(angle)\n",
        "\n",
        "def vector(start, end):\n",
        "    return np.array([end['x'] - start['x'], end['y'] - start['y'], end['z'] - start['z']])\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    return angle_between(vector(b, a), vector(b, c))\n",
        "\n",
        "def midpoint(a, b):\n",
        "    return (a + b) / 2\n",
        "\n",
        "def vector_plane_project(vector, plane_normal):\n",
        "    vector_proj = vector - np.dot(vector, plane_normal) * plane_normal\n",
        "    vector_proj = normalize(vector_proj)\n",
        "    return vector_proj\n",
        "\n",
        "\n",
        "# Function to compute 23 specific angles from 33 keypoints\n",
        "def calculate_angles(landmarks):\n",
        "    \"\"\"Compute 20 joint-angle features from Mediapipe landmarks.\"\"\"\n",
        "    if not landmarks or len(landmarks) < 33:\n",
        "        return [0.0] * NUM_FEATURES\n",
        "    # Define landmark indices\n",
        "    LEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\n",
        "    LEFT_ELBOW, RIGHT_ELBOW = 13, 14\n",
        "    LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
        "    LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
        "    LEFT_PINKY, RIGHT_PINKY = 17, 18\n",
        "    LEFT_INDEX,RIGHT_INDEX = 19, 20\n",
        "    LEFT_HIP, RIGHT_HIP = 23, 24\n",
        "    LEFT_KNEE, RIGHT_KNEE = 25, 26\n",
        "    LEFT_ANKLE, RIGHT_ANKLE = 27, 28\n",
        "\n",
        "    # Estimating Primary Vectors\n",
        "    L_shoulder = point(landmarks[LEFT_SHOULDER])  # LEFT_SHOULDER\n",
        "    R_shoulder = point(landmarks[RIGHT_SHOULDER])  # RIGHT_SHOULDER\n",
        "    L_hip = point(landmarks[LEFT_HIP])       # LEFT_HIP\n",
        "    R_hip = point(landmarks[RIGHT_HIP])       # RIGHT_HIP\n",
        "    L_ankle = point(landmarks[LEFT_ANKLE])       # LEFT_ANKLE\n",
        "    R_ankle = point(landmarks[RIGHT_ANKLE])       # RIGHT_ANKLE\n",
        "\n",
        "    # Midpoints\n",
        "    mid_shoulder = midpoint(L_shoulder, R_shoulder)\n",
        "    mid_hip = midpoint(L_hip, R_hip)\n",
        "\n",
        "    # Local torso axes\n",
        "    horizontal = normalize(R_shoulder - L_shoulder)           # Local X (shoulder width)\n",
        "    upward = normalize(mid_shoulder - mid_hip)          # Local Y (torso up)\n",
        "    forward = normalize(np.cross(horizontal, upward))          # Local Z (front-back normal to torso plane)\n",
        "    # Re-orthogonalize Y just in case (to maintain right-handed frame)\n",
        "    upward = normalize(np.cross(forward, horizontal))\n",
        "\n",
        "    # Torso flexion\n",
        "    mid_ankle = midpoint(L_ankle, R_ankle)\n",
        "    torso = mid_shoulder - mid_hip\n",
        "    gravity = mid_ankle - mid_hip\n",
        "    torso_flexion = angle_between(torso, gravity)\n",
        "\n",
        "    # Elbow\n",
        "    left_elbow = calculate_angle(landmarks[LEFT_SHOULDER], landmarks[LEFT_ELBOW], landmarks[LEFT_WRIST])\n",
        "    right_elbow = calculate_angle(landmarks[RIGHT_SHOULDER], landmarks[RIGHT_ELBOW], landmarks[RIGHT_WRIST])\n",
        "\n",
        "    # Shoulder\n",
        "    left_shoulder = calculate_angle(landmarks[LEFT_HIP], landmarks[LEFT_SHOULDER], landmarks[LEFT_ELBOW])\n",
        "    right_shoulder = calculate_angle(landmarks[RIGHT_HIP], landmarks[RIGHT_SHOULDER], landmarks[RIGHT_ELBOW])\n",
        "\n",
        "    # Spinal alignment\n",
        "    left_hand = midpoint(point(landmarks[LEFT_PINKY]), point(landmarks[LEFT_INDEX]))\n",
        "    left_wrist = angle_between(left_hand - point(landmarks[LEFT_WRIST]),\n",
        "                               point(landmarks[LEFT_ELBOW]) - point(landmarks[LEFT_WRIST]))\n",
        "    right_hand = midpoint(point(landmarks[RIGHT_PINKY]), point(landmarks[RIGHT_INDEX]))\n",
        "    right_wrist = angle_between(right_hand - point(landmarks[RIGHT_WRIST]),\n",
        "                                point(landmarks[RIGHT_ELBOW]) - point(landmarks[RIGHT_WRIST]))\n",
        "\n",
        "    # Scapular upward rotation\n",
        "    left_scapular_upward_rotation = angle_between(vector(landmarks[LEFT_HIP], landmarks[LEFT_SHOULDER]),\n",
        "                                                  vector(landmarks[LEFT_HIP], landmarks[RIGHT_HIP]))\n",
        "\n",
        "    right_scapular_upward_rotation = angle_between(vector(landmarks[RIGHT_HIP], landmarks[RIGHT_SHOULDER]),\n",
        "                                                   vector(landmarks[RIGHT_HIP], landmarks[LEFT_HIP]))\n",
        "\n",
        "    # Shoulder abduction/extension\n",
        "    v_arm_left = vector(landmarks[LEFT_SHOULDER],  landmarks[LEFT_ELBOW])\n",
        "    f_proj_v_arm_left = vector_plane_project(v_arm_left, forward)\n",
        "    f_left_shoulder_abduction = angle_between(f_proj_v_arm_left, upward)\n",
        "    h_proj_v_arm_left = vector_plane_project(v_arm_left, upward)\n",
        "    h_left_shoulder_adduction = angle_between(h_proj_v_arm_left, forward)\n",
        "    s_proj_v_arm_left = vector_plane_project(v_arm_left, horizontal)\n",
        "    left_shoulder_extension = angle_between(s_proj_v_arm_left, upward)\n",
        "\n",
        "    v_arm_right = vector(landmarks[RIGHT_SHOULDER],  landmarks[RIGHT_ELBOW])\n",
        "    f_proj_v_arm_right = vector_plane_project(v_arm_right, forward)\n",
        "    f_right_shoulder_abduction = angle_between(f_proj_v_arm_right, upward)\n",
        "    h_proj_v_arm_right = vector_plane_project(v_arm_right, upward)\n",
        "    h_right_shoulder_adduction = angle_between(h_proj_v_arm_right, forward)\n",
        "    s_proj_v_arm_right = vector_plane_project(v_arm_right, horizontal)\n",
        "    right_shoulder_extension = angle_between(s_proj_v_arm_right, upward)\n",
        "\n",
        "    # Knee\n",
        "    left_knee = calculate_angle(landmarks[LEFT_HIP], landmarks[LEFT_KNEE], landmarks[LEFT_ANKLE])\n",
        "    right_knee = calculate_angle(landmarks[RIGHT_HIP], landmarks[RIGHT_KNEE], landmarks[RIGHT_ANKLE])\n",
        "\n",
        "    # Hip\n",
        "    left_hip = calculate_angle(landmarks[LEFT_SHOULDER], landmarks[LEFT_HIP], landmarks[LEFT_KNEE])\n",
        "    right_hip = calculate_angle(landmarks[RIGHT_SHOULDER], landmarks[RIGHT_HIP], landmarks[RIGHT_KNEE])\n",
        "\n",
        "    # Ankle dorsiflexion\n",
        "    v_shin_left = vector(landmarks[LEFT_KNEE],  landmarks[LEFT_ANKLE])\n",
        "    s_proj_v_shin_left = vector_plane_project(v_shin_left, horizontal)\n",
        "    left_ankle_dorsiflexion = angle_between(s_proj_v_shin_left, upward)\n",
        "\n",
        "    v_shin_right = vector(landmarks[RIGHT_KNEE],  landmarks[RIGHT_ANKLE])\n",
        "    s_proj_v_shin_right = vector_plane_project(v_shin_right, horizontal)\n",
        "    right_ankle_dorsiflexion = angle_between(s_proj_v_shin_right, upward)\n",
        "\n",
        "    # Left knee valgus indicator\n",
        "    f_proj_left_hip = vector_plane_project(point(landmarks[LEFT_HIP]), forward)\n",
        "    f_proj_left_knee = vector_plane_project(point(landmarks[LEFT_KNEE]), forward)\n",
        "    f_proj_left_ankle = vector_plane_project(point(landmarks[LEFT_ANKLE]), forward)\n",
        "    left_knee_valgus_indicator = angle_between(f_proj_left_hip - f_proj_left_knee, f_proj_left_ankle - f_proj_left_knee)\n",
        "    # Right knee valgus indicator\n",
        "    f_proj_right_hip = vector_plane_project(point(landmarks[RIGHT_HIP]), forward)\n",
        "    f_proj_right_knee = vector_plane_project(point(landmarks[RIGHT_KNEE]), forward)\n",
        "    f_proj_right_ankle = vector_plane_project(point(landmarks[RIGHT_ANKLE]), forward)\n",
        "    right_knee_valgus_indicator = angle_between(f_proj_right_hip - f_proj_right_knee, f_proj_right_ankle - f_proj_right_knee)\n",
        "\n",
        "    angles = {\n",
        "        'torso flexion': torso_flexion,\n",
        "        'left elbow': left_elbow,\n",
        "        'right elbow': right_elbow,\n",
        "        'left shoulder': left_shoulder,\n",
        "        'right shoulder': right_shoulder,\n",
        "        'left wrist': left_wrist,\n",
        "        'right wrist': right_wrist,\n",
        "        'left frontal shoulder abduction': f_left_shoulder_abduction,\n",
        "        'right frontal shoulder abduction': f_right_shoulder_abduction,\n",
        "        'left scapular upward rotation': left_scapular_upward_rotation,\n",
        "        'right scapular upward rotation': right_scapular_upward_rotation,\n",
        "        'left knee': left_knee,\n",
        "        'right knee': right_knee,\n",
        "        'left hip': left_hip,\n",
        "        'right hip': right_hip,\n",
        "        'left ankle dorsiflexion': left_ankle_dorsiflexion,\n",
        "        'right ankle dorsiflexion': right_ankle_dorsiflexion,\n",
        "        'left knee valgus indicator': left_knee_valgus_indicator,\n",
        "        'right knee valgus indicator': right_knee_valgus_indicator,\n",
        "        'left horizontal shoulder adduction': h_left_shoulder_adduction,\n",
        "        'right horizontal shoulder adduction': h_right_shoulder_adduction,\n",
        "        'left shoulder extension': left_shoulder_extension,\n",
        "        'right shoulder extension': right_shoulder_extension,\n",
        "    }\n",
        "\n",
        "    return angles\n",
        "\n",
        "def normalize_landmarks(landmarks):\n",
        "    if not landmarks or len(landmarks) < 33:\n",
        "        return []\n",
        "\n",
        "    # Body center calculation (using hips)\n",
        "    left_hip = landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value]\n",
        "    right_hip = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value]\n",
        "    center = {\n",
        "        'x': (left_hip.x + right_hip.x) / 2,\n",
        "        'y': (left_hip.y + right_hip.y) / 2,\n",
        "        'z': (left_hip.z + right_hip.z) / 2\n",
        "    }\n",
        "\n",
        "    # Scale calculation (shoulder width)\n",
        "    left_shoulder = landmarks[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "    right_shoulder = landmarks[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "    scale = abs(left_shoulder.x - right_shoulder.x) or 1e-5\n",
        "\n",
        "    normalized = []\n",
        "    for lm in landmarks:\n",
        "        normalized.append({\n",
        "            'x': (lm.x - center['x']) / scale,\n",
        "            'y': (lm.y - center['y']) / scale,\n",
        "            'z': lm.z - center['z'],\n",
        "            'visibility': lm.visibility\n",
        "        })\n",
        "    return normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd6aa22-1762-4291-8f56-bcd9babd1f0d",
      "metadata": {
        "id": "bfd6aa22-1762-4291-8f56-bcd9babd1f0d",
        "outputId": "4b42c2ab-1c4b-4190-ee3d-c882c964e761"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████| 140/140 [00:32<00:00,  4.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total frames: 140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "coach_video = 'videos/romanian deadlift/romanian deadlift_10.mp4'\n",
        "athlete_video = 'videos/deadlift/deadlift_25.mp4'\n",
        "poser = mp.solutions.pose.Pose(\n",
        "            static_image_mode=False,\n",
        "            model_complexity=2,\n",
        "            min_detection_confidence=0.7,\n",
        "            min_tracking_confidence=0.7\n",
        "        )\n",
        "\n",
        "cap = cv2.VideoCapture(coach_video)\n",
        "T = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "coach_angles = np.empty((T, 23), np.uint8)\n",
        "for t in tqdm(range(T)):\n",
        "    success, frame = cap.read()\n",
        "    coach_frames[t, :, :, :] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    result = poser.process(frame)\n",
        "    landmarks = result.pose_landmarks.landmark\n",
        "    angles = calculate_angles(normalize_landmarks(landmarks))\n",
        "    names = sorted(list(angles.keys()))\n",
        "    angles = [angles[name] for name in names]\n",
        "    coach_angles[t, :] = angles\n",
        "print(\"Total frames:\", frame_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be57b2d8-2b22-46c6-aa17-2e7f2915cf72",
      "metadata": {
        "id": "be57b2d8-2b22-46c6-aa17-2e7f2915cf72",
        "outputId": "d23d51d2-f68a-4a54-d232-f7e5a0195960"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "[0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1]\n"
          ]
        }
      ],
      "source": [
        "coach = np.empty((1, classifier_model.input_shape[1], classifier_model.input_shape[2]), np.float32)\n",
        "if coach_angles.shape[0] < classifier_model.input_shape[1]:\n",
        "    coach[0, :coach_angles.shape[0], :] = coach_angles[:, :]\n",
        "    coach[0, coach_angles.shape[0]:, :] = 0\n",
        "else:\n",
        "    coach[0, :, :] = coach_angles[:classifier_model.input_shape[1], :]\n",
        "usefulness = np.where(classifier_model.predict(coach)[0] < 0.5, 0, 1)\n",
        "print(usefulness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d41465-eed3-4d4b-983b-c0047ebccea5",
      "metadata": {
        "id": "69d41465-eed3-4d4b-983b-c0047ebccea5",
        "outputId": "4db85512-eb10-4bb2-b6ea-22598921ff74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████| 938/938 [01:44<00:00,  8.95it/s]\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture(athlete_video)\n",
        "T = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "athlete_frames = np.empty((T, H, W, 3), np.uint8)\n",
        "athlete_angles = np.empty((T, 23), np.uint8)\n",
        "athlete_landmarks = []\n",
        "for t in tqdm(range(T)):\n",
        "    success, frame = cap.read()\n",
        "    if success:\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        result = poser.process(frame)\n",
        "        landmarks = result.pose_landmarks.landmark\n",
        "        angles = calculate_angles(normalize_landmarks(landmarks))\n",
        "        names = sorted(list(angles.keys()))\n",
        "        angles = [angles[name] for name in names]\n",
        "        athlete_frames[t, :, :, :] = frame\n",
        "        athlete_angles[t, :] = angles\n",
        "        athlete_landmarks.append(landmarks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44418621-c69b-4079-96d3-78bedae6ed7c",
      "metadata": {
        "id": "44418621-c69b-4079-96d3-78bedae6ed7c",
        "outputId": "36ed57d5-4975-4a14-eb25-d8da5038fc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step\n"
          ]
        }
      ],
      "source": [
        "coach = np.empty((athlete_angles.shape[0], siamese_model.input_shape[0][1], siamese_model.input_shape[0][2]), np.float32)\n",
        "if coach_angles.shape[0] < classifier_model.input_shape[1]:\n",
        "    for t in range(athlete_angles.shape[0]):\n",
        "        coach[t, :coach_angles.shape[0], :] = coach_angles[:, :]\n",
        "        coach[t, coach_angles.shape[0]:, :] = 0\n",
        "else:\n",
        "    for t in range(athlete_angles.shape[0]):\n",
        "        coach[t, :, :] = coach_angles[:siamese_model.input_shape[0][1], :]\n",
        "\n",
        "athlete = np.empty((athlete_angles.shape[0], siamese_model.input_shape[1][1], siamese_model.input_shape[1][2]), np.float32)\n",
        "for t in range(athlete_angles.shape[0]):\n",
        "    if t < siamese_model.input_shape[1][1]:\n",
        "        athlete[t, :t, :] = athlete_angles[:t, :]\n",
        "        athlete[t, t:, :] = 0\n",
        "    else:\n",
        "        athlete[t, :, :] = athlete_angles[t-siamese_model.input_shape[1][1]:t, :]\n",
        "\n",
        "similarity = siamese_model.predict([coach, athlete])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c525383f-f909-4e79-9e86-ffa8294fb340",
      "metadata": {
        "id": "c525383f-f909-4e79-9e86-ffa8294fb340",
        "outputId": "d4d92b96-6d8a-4f15-f476-4896d5a4f80e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████| 938/938 [00:20<00:00, 46.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video saved as output_video.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "# Function to compute 23 specific angles from 33 keypoints\n",
        "def get_angle_points(landmarks):\n",
        "    \"\"\"Compute 20 joint-angle features from Mediapipe landmarks.\"\"\"\n",
        "    if not landmarks or len(landmarks) < 33:\n",
        "        return [0.0] * NUM_FEATURES\n",
        "    # Define landmark indices\n",
        "    LEFT_SHOULDER, RIGHT_SHOULDER = 11, 12\n",
        "    LEFT_ELBOW, RIGHT_ELBOW = 13, 14\n",
        "    LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
        "    LEFT_WRIST, RIGHT_WRIST = 15, 16\n",
        "    LEFT_PINKY, RIGHT_PINKY = 17, 18\n",
        "    LEFT_INDEX,RIGHT_INDEX = 19, 20\n",
        "    LEFT_HIP, RIGHT_HIP = 23, 24\n",
        "    LEFT_KNEE, RIGHT_KNEE = 25, 26\n",
        "    LEFT_ANKLE, RIGHT_ANKLE = 27, 28\n",
        "\n",
        "    # Estimating Primary Vectors\n",
        "    L_shoulder = point(landmarks[LEFT_SHOULDER])  # LEFT_SHOULDER\n",
        "    R_shoulder = point(landmarks[RIGHT_SHOULDER])  # RIGHT_SHOULDER\n",
        "    L_hip = point(landmarks[LEFT_HIP])       # LEFT_HIP\n",
        "    R_hip = point(landmarks[RIGHT_HIP])       # RIGHT_HIP\n",
        "    L_ankle = point(landmarks[LEFT_ANKLE])       # LEFT_ANKLE\n",
        "    R_ankle = point(landmarks[RIGHT_ANKLE])       # RIGHT_ANKLE\n",
        "\n",
        "    # Midpoints\n",
        "    mid_shoulder = midpoint(L_shoulder, R_shoulder)\n",
        "    mid_hip = midpoint(L_hip, R_hip)\n",
        "\n",
        "    # Local torso axes\n",
        "    horizontal = normalize(R_shoulder - L_shoulder)           # Local X (shoulder width)\n",
        "    upward = normalize(mid_shoulder - mid_hip)          # Local Y (torso up)\n",
        "    forward = normalize(np.cross(horizontal, upward))          # Local Z (front-back normal to torso plane)\n",
        "    # Re-orthogonalize Y just in case (to maintain right-handed frame)\n",
        "    upward = normalize(np.cross(forward, horizontal))\n",
        "\n",
        "    # Torso flexion\n",
        "    mid_ankle = midpoint(L_ankle, R_ankle)\n",
        "    torso = mid_shoulder - mid_hip\n",
        "    gravity = mid_ankle - mid_hip\n",
        "    torso_flexion = (mid_shoulder, mid_hip, mid_ankle)\n",
        "\n",
        "    # Elbow\n",
        "    left_elbow = (point(landmarks[LEFT_WRIST]), point(landmarks[LEFT_ELBOW]), point(landmarks[LEFT_SHOULDER]))\n",
        "    right_elbow = (point(landmarks[RIGHT_WRIST]), point(landmarks[RIGHT_ELBOW]), point(landmarks[RIGHT_SHOULDER]))\n",
        "\n",
        "    # Shoulder\n",
        "    left_shoulder = (point(landmarks[LEFT_HIP]), point(landmarks[LEFT_SHOULDER]), point(landmarks[LEFT_ELBOW]))\n",
        "    right_shoulder = (point(landmarks[RIGHT_HIP]), point(landmarks[RIGHT_SHOULDER]), point(landmarks[RIGHT_ELBOW]))\n",
        "\n",
        "    # Wrist\n",
        "    left_hand = midpoint(point(landmarks[LEFT_PINKY]), point(landmarks[LEFT_INDEX]))\n",
        "    left_wrist = (left_hand, point(landmarks[LEFT_WRIST]), point(landmarks[LEFT_ELBOW]))\n",
        "    right_hand = midpoint(point(landmarks[RIGHT_PINKY]), point(landmarks[RIGHT_INDEX]))\n",
        "    right_wrist = (right_hand, point(landmarks[RIGHT_WRIST]), point(landmarks[RIGHT_ELBOW]))\n",
        "\n",
        "    # Scapular upward rotation\n",
        "    left_scapular_upward_rotation = (point(landmarks[LEFT_SHOULDER]), point(landmarks[LEFT_HIP]),  point(landmarks[RIGHT_HIP]))\n",
        "\n",
        "    right_scapular_upward_rotation = (point(landmarks[RIGHT_SHOULDER]), point(landmarks[RIGHT_HIP]),  point(landmarks[LEFT_HIP]))\n",
        "\n",
        "    # Shoulder abduction/extension\n",
        "    shoulder = point(landmarks[LEFT_SHOULDER])\n",
        "    v_arm_left = vector(landmarks[LEFT_SHOULDER],  landmarks[LEFT_ELBOW])\n",
        "    f_proj_v_arm_left = vector_plane_project(v_arm_left, forward)\n",
        "    f_left_shoulder_abduction = (shoulder + f_proj_v_arm_left, shoulder, shoulder + upward)\n",
        "    h_proj_v_arm_left = vector_plane_project(v_arm_left, upward)\n",
        "    h_left_shoulder_adduction = (shoulder + h_proj_v_arm_left, shoulder, shoulder + forward)\n",
        "    s_proj_v_arm_left = vector_plane_project(v_arm_left, horizontal)\n",
        "    left_shoulder_extension = (shoulder + s_proj_v_arm_left, shoulder, shoulder + upward)\n",
        "\n",
        "    shoulder = point(landmarks[RIGHT_SHOULDER])\n",
        "    v_arm_right = vector(landmarks[RIGHT_SHOULDER],  landmarks[RIGHT_ELBOW])\n",
        "    f_proj_v_arm_right = vector_plane_project(v_arm_right, forward)\n",
        "    f_right_shoulder_abduction = (shoulder + f_proj_v_arm_right, shoulder, shoulder + upward)\n",
        "    h_proj_v_arm_right = vector_plane_project(v_arm_right, upward)\n",
        "    h_right_shoulder_adduction = (shoulder + h_proj_v_arm_right, shoulder, shoulder + forward)\n",
        "    s_proj_v_arm_right = vector_plane_project(v_arm_right, horizontal)\n",
        "    right_shoulder_extension = (shoulder + s_proj_v_arm_right, shoulder, shoulder + upward)\n",
        "\n",
        "    # Knee\n",
        "    left_knee = (point(landmarks[LEFT_HIP]), point(landmarks[LEFT_KNEE]), point(landmarks[LEFT_ANKLE]))\n",
        "    right_knee = (point(landmarks[RIGHT_HIP]), point(landmarks[RIGHT_KNEE]), point(landmarks[RIGHT_ANKLE]))\n",
        "\n",
        "    # Hip\n",
        "    left_hip = (point(landmarks[LEFT_SHOULDER]), point(landmarks[LEFT_HIP]), point(landmarks[LEFT_KNEE]))\n",
        "    right_hip = (point(landmarks[RIGHT_SHOULDER]), point(landmarks[RIGHT_HIP]), point(landmarks[RIGHT_KNEE]))\n",
        "\n",
        "    # Ankle dorsiflexion\n",
        "    ankle = point(landmarks[LEFT_ANKLE])\n",
        "    v_shin_left = vector(landmarks[LEFT_KNEE],  landmarks[LEFT_ANKLE])\n",
        "    s_proj_v_shin_left = vector_plane_project(v_shin_left, horizontal)\n",
        "    left_ankle_dorsiflexion = (ankle + s_proj_v_shin_left, ankle, ankle + upward)\n",
        "\n",
        "    ankle = point(landmarks[RIGHT_ANKLE])\n",
        "    v_shin_right = vector(landmarks[RIGHT_KNEE],  landmarks[RIGHT_ANKLE])\n",
        "    s_proj_v_shin_right = vector_plane_project(v_shin_right, horizontal)\n",
        "    right_ankle_dorsiflexion = (ankle + s_proj_v_shin_right, ankle, ankle + upward)\n",
        "\n",
        "    # Left knee valgus indicator\n",
        "    knee = point(landmarks[LEFT_KNEE])\n",
        "    f_proj_left_hip = vector_plane_project(point(landmarks[LEFT_HIP]), forward)\n",
        "    f_proj_left_knee = vector_plane_project(point(landmarks[LEFT_KNEE]), forward)\n",
        "    f_proj_left_ankle = vector_plane_project(point(landmarks[LEFT_ANKLE]), forward)\n",
        "    left_knee_valgus_indicator = (knee + f_proj_left_hip - f_proj_left_knee, knee, knee + f_proj_left_ankle - f_proj_left_knee)\n",
        "    # Right knee valgus indicator\n",
        "    knee = point(landmarks[RIGHT_KNEE])\n",
        "    f_proj_right_hip = vector_plane_project(point(landmarks[RIGHT_HIP]), forward)\n",
        "    f_proj_right_knee = vector_plane_project(point(landmarks[RIGHT_KNEE]), forward)\n",
        "    f_proj_right_ankle = vector_plane_project(point(landmarks[RIGHT_ANKLE]), forward)\n",
        "    right_knee_valgus_indicator = (knee + f_proj_right_hip - f_proj_right_knee, knee, knee + f_proj_right_ankle - f_proj_right_knee)\n",
        "\n",
        "    angles = {\n",
        "        'torso flexion': torso_flexion,\n",
        "        'left elbow': left_elbow,\n",
        "        'right elbow': right_elbow,\n",
        "        'left shoulder': left_shoulder,\n",
        "        'right shoulder': right_shoulder,\n",
        "        'left wrist': left_wrist,\n",
        "        'right wrist': right_wrist,\n",
        "        'left frontal shoulder abduction': f_left_shoulder_abduction,\n",
        "        'right frontal shoulder abduction': f_right_shoulder_abduction,\n",
        "        'left scapular upward rotation': left_scapular_upward_rotation,\n",
        "        'right scapular upward rotation': right_scapular_upward_rotation,\n",
        "        'left knee': left_knee,\n",
        "        'right knee': right_knee,\n",
        "        'left hip': left_hip,\n",
        "        'right hip': right_hip,\n",
        "        'left ankle dorsiflexion': left_ankle_dorsiflexion,\n",
        "        'right ankle dorsiflexion': right_ankle_dorsiflexion,\n",
        "        'left knee valgus indicator': left_knee_valgus_indicator,\n",
        "        'right knee valgus indicator': right_knee_valgus_indicator,\n",
        "        'left horizontal shoulder adduction': h_left_shoulder_adduction,\n",
        "        'right horizontal shoulder adduction': h_right_shoulder_adduction,\n",
        "        'left shoulder extension': left_shoulder_extension,\n",
        "        'right shoulder extension': right_shoulder_extension,\n",
        "    }\n",
        "    for key in angles:\n",
        "        angles[key] = normalize_angle_points(angles[key])\n",
        "    return angles\n",
        "\n",
        "def standardize_landmarks(landmarks):\n",
        "    if not landmarks or len(landmarks) < 33:\n",
        "        return []\n",
        "    normalized = []\n",
        "    for lm in landmarks:\n",
        "        normalized.append({\n",
        "            'x': lm.x,\n",
        "            'y': lm.y,\n",
        "            'z': lm.z,\n",
        "            'visibility': lm.visibility\n",
        "        })\n",
        "    return normalized\n",
        "\n",
        "def normalize_angle_points(points):\n",
        "    a, b, c = points\n",
        "    la = np.linalg.norm(a - b)\n",
        "    lc = np.linalg.norm(c - b)\n",
        "    lm = max(la, lc)\n",
        "    d = b + (a - b) * lm / la * 0.1\n",
        "    e = b + (c - b) * lm / lc * 0.1\n",
        "    a = b + (a - b) * lm / la * 0.3\n",
        "    c = b + (c - b) * lm / lc * 0.3\n",
        "    return b, a, c, d, e\n",
        "\n",
        "def draw_pi_slice(draw, center, point1, point2, fill):\n",
        "    radius = np.linalg.norm(point1 - center)\n",
        "    bbox = [\n",
        "        center[0] - radius, center[1] - radius,\n",
        "        center[0] + radius, center[1] + radius\n",
        "    ]\n",
        "\n",
        "    def angle_between(p, center):\n",
        "        dx = p[0] - center[0]\n",
        "        dy = p[1] - center[1]\n",
        "        angle = np.degrees(np.arctan2(dy, dx)) % 360\n",
        "        return angle\n",
        "\n",
        "    start_angle = angle_between(point1, center)\n",
        "    end_angle = angle_between(point2, center)\n",
        "    if (end_angle - start_angle) % 360 > 180:\n",
        "        end_angle, start_angle = start_angle, end_angle\n",
        "    draw.pieslice(bbox, start=start_angle, end=end_angle, fill=fill)\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "w, h = athlete_frames.shape[2], athlete_frames.shape[1]\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID'\n",
        "out = cv2.VideoWriter('output_video.mp4', fourcc, FPS, (w, h))\n",
        "\n",
        "# Write images to video\n",
        "for t in tqdm(range(athlete_frames.shape[0])):\n",
        "    landmarks = athlete_landmarks[t]\n",
        "    frame = Image.fromarray(athlete_frames[t])\n",
        "    draw = ImageDraw.Draw(frame)\n",
        "    angle_points = get_angle_points(standardize_landmarks(landmarks))\n",
        "    for i in range(len(angle_points)):\n",
        "        if usefulness[i] >= 0.5:\n",
        "            a, b, c, d, e = angle_points[names[i]]\n",
        "            r, g = int(255 * (1 - similarity[t, i])), int(255 * similarity[t, i])\n",
        "            draw.line([a[0]*w, a[1]*h, b[0]*w, b[1]*h], fill=(r, g, 0), width=5)\n",
        "            draw.line([a[0]*w, a[1]*h, c[0]*w, c[1]*h], fill=(r, g, 0), width=5)\n",
        "            draw_pi_slice(draw,\n",
        "                          np.array([a[0]*w, a[1]*h]),\n",
        "                          np.array([d[0]*w, d[1]*h]),\n",
        "                          np.array([e[0]*w, e[1]*h]),\n",
        "                          (r, g, 0))\n",
        "    out.write(cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB))  # Add frame\n",
        "\n",
        "# Release everything\n",
        "out.release()\n",
        "print(\"Video saved as output_video.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4435c9d-3b88-4706-9a3e-af30cfe3ee32",
      "metadata": {
        "id": "e4435c9d-3b88-4706-9a3e-af30cfe3ee32"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}