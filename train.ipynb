{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_HAe5VzbUd7"
      },
      "source": [
        "# Get the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA7POIpDbUd8",
        "outputId": "7e607803-469a-4636-9e3e-f1bd119c62f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sitrex' already exists and is not an empty directory.\n",
            "/content/sitrex\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/asellam/sitrex.git\n",
        "%cd sitrex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QTceCwapbUd_",
        "outputId": "cea40309-b3d8-49ab-fa1e-bcc13c1ece1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0,>=1.23 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.16.1)\n",
            "Requirement already satisfied: tqdm>=4.65 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
            "Requirement already satisfied: kagglehub>=0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.3.12)\n",
            "Collecting mediapipe==0.10.21 (from -r requirements.txt (line 15))\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.21->-r requirements.txt (line 15))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.21->-r requirements.txt (line 15))\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (0.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 9)) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 12)) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 12)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 12)) (2.32.4)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21->-r requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21->-r requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe==0.10.21->-r requirements.txt (line 15))\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (2025.8.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.21->-r requirements.txt (line 15)) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "0d90ab6d48a648e5b4580b9e9760110f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6fubIm76ZdW"
      },
      "source": [
        "# Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZZt0APPizmv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cd0093-b820-4ab9-88b9-a59973d864e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/abdellah213/sitrex-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 418M/418M [00:03<00:00, 139MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download('abdellah213/sitrex-dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbuBNCCw69KI"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p0IEFaNKzz3F"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sitrex.preprocessing import load_dataset, preprocess_data, SimilarityDataset, UsefulnessDataset\n",
        "from sitrex.model import angle_usefulness_model, angle_similarity_model, TQDMProgressBar\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "# Define constants\n",
        "DATASET_PATH = os.path.join(path, 'dataset')\n",
        "# EarlyStop save path for the Siamese Transformer\n",
        "MODEL_SAVE_PATH_SIMILARITY = './angle_similarity_model.keras'\n",
        "# EarlyStop save path for the Angle Usefulness Transformer\n",
        "MODEL_SAVE_PATH_USEFULNESS = './angle_usefulness_model.keras'\n",
        "# Maximum sequence length (L)\n",
        "MAXLEN = 100\n",
        "# Batch Size\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELxuOym7bUeA"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMNQw9Ql7_YW",
        "outputId": "3ae86d89-b06c-45f6-f693-31913283faad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset: 100%|██████████| 727/727 [03:21<00:00,  3.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 727 sequences with labels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "all_sequences, labels, exercise_angles = load_dataset(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Fvh4Ak7Hnw"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C3XNnuaE8P0H"
      },
      "outputs": [],
      "source": [
        "processed_sequences, numerical_labels, label_angles = preprocess_data(all_sequences, labels, exercise_angles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1-x8hJi8Wxz"
      },
      "source": [
        "# Tensorflow Dataset and Model Routines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfoMcUK9ceH"
      },
      "source": [
        "# K-Fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "45B5HZpO9Vl9"
      },
      "outputs": [],
      "source": [
        "def split_by_fold(data_x, data_y, folds, test):\n",
        "    train_x, test_x, train_y, test_y = [], [], [], []\n",
        "    for fold, split in enumerate(folds):\n",
        "        for sample in split:\n",
        "            if fold == test:\n",
        "                test_x.append(data_x[sample])\n",
        "                test_y.append(data_y[sample])\n",
        "            else:\n",
        "                train_x.append(data_x[sample])\n",
        "                train_y.append(data_y[sample])\n",
        "    return train_x, test_x, train_y, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOFTlTJZ9wRL",
        "outputId": "a529c2d4-e937-47aa-8678-f0bd8888654f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([513, 693, 377,  33,  63, 467, 346, 511, 148, 388, 174,  65, 469,\n",
            "       428, 707, 350, 210,  72, 449,  78,  54,  39,  97, 211,  81, 557,\n",
            "       629, 443, 514,  10, 361, 319, 676, 673, 231, 363, 158, 462, 296,\n",
            "       578, 723, 396,  86, 329, 597, 235, 351, 568, 299, 685, 165, 571,\n",
            "       164, 223, 539, 399, 281, 227, 714, 620, 199, 705, 155,  49, 332,\n",
            "       101, 477, 234, 437, 196,  23, 266,  77, 212, 198, 109, 713, 327,\n",
            "       528, 336, 209,  30, 602, 674, 480, 554, 393, 547, 118, 609, 133,\n",
            "       688,  84,  79, 213, 433, 448, 181,  31, 254, 598, 215, 412, 591,\n",
            "       275,  55, 679, 594, 691, 300,  76,   2, 506, 464, 192,  60, 722,\n",
            "       120, 362, 429, 314, 218, 220, 634, 260, 405, 244, 426, 431,  90,\n",
            "       331, 538,  69, 204, 131,  44,  70, 349, 717, 569, 135, 601, 239,\n",
            "       579, 642, 432]), array([136, 286,   6, 657, 587, 334, 250, 145, 338, 132, 306, 660,  41,\n",
            "       108, 292,  56, 417, 690, 333, 537,  24, 404, 465, 666, 311, 457,\n",
            "       110,  82,  51, 394, 582, 497, 290, 265, 501, 650, 603,  18, 342,\n",
            "       369, 694,  83,  61, 533, 625,  29, 584, 710, 678, 367, 182, 482,\n",
            "       604, 408, 585, 381, 208, 264, 436, 589, 176, 294, 259, 530, 248,\n",
            "       137, 409, 680, 163, 425, 637, 483, 581, 324, 328, 450, 104, 114,\n",
            "       652, 575, 651, 424,  92,   7,  89, 551, 356, 479, 544, 515, 453,\n",
            "       247, 140,  28,  43,  42,  73, 167, 398, 635,  66,  11, 335, 627,\n",
            "       178, 645, 456, 446, 355, 526, 177, 622, 628, 257, 291,  15, 422,\n",
            "       256, 302, 318, 623, 451, 340, 430, 375,   9, 249,  22, 221, 655,\n",
            "       711, 687, 607, 203, 440,  93, 326, 535, 416, 284, 184, 586, 323,\n",
            "       153,  75, 371]), array([277,  68, 420, 188, 271, 236,  88, 615, 117, 125, 596, 289, 238,\n",
            "         0, 718, 344, 632, 380, 278, 116, 228, 633, 357, 274, 487, 144,\n",
            "       423, 542, 656, 529, 268, 512, 307, 310,  46, 261, 195, 726, 608,\n",
            "       107, 507, 543, 360, 100, 390, 649, 527, 704, 179, 304, 352, 658,\n",
            "       698, 149, 124, 599, 630, 185, 708, 725, 382, 716, 500, 321, 353,\n",
            "       669, 142, 141, 434, 320,  19, 172, 590, 312, 675,  12, 305, 354,\n",
            "        25, 541, 169,  38, 445, 245, 298, 478, 272, 154, 126, 516, 341,\n",
            "       287, 113, 485, 173, 359, 395,  57, 519, 222, 280,  17, 447, 322,\n",
            "       255, 699, 696, 190, 595, 641, 411,  94, 180, 301, 470, 534, 665,\n",
            "       558, 640, 473,   5, 712,  45, 439, 548, 171,  16,  48, 702, 664,\n",
            "         3, 494, 667, 316, 668, 283,  96, 285, 481, 225,  26, 570, 263,\n",
            "        50, 364, 229]), array([ 37, 157, 237, 697, 374, 370, 442, 175, 616, 127, 536, 194, 618,\n",
            "       626, 468, 490, 493, 701, 605, 624,  67, 486, 168, 444, 639, 162,\n",
            "       309, 193, 518, 365, 383, 654, 525, 152, 495, 517, 522, 226, 606,\n",
            "       580, 103, 421, 419, 567, 550,  74, 115, 407, 721, 119,  53, 151,\n",
            "       403, 683, 207, 503, 521, 611, 695,   8, 588,  36, 452, 139, 253,\n",
            "       303, 610, 549, 368,  59, 499, 684, 111, 523, 531, 262, 297, 414,\n",
            "       150, 576, 644, 488, 147, 146, 532, 672, 545, 348, 463, 325, 186,\n",
            "       123, 617, 143, 692, 197, 279, 293, 400, 122, 183, 202, 438, 246,\n",
            "       415, 643, 129, 402, 572, 671, 219, 659, 552, 662, 559, 386, 703,\n",
            "       509, 267, 441, 496, 112, 232, 631, 373, 233, 583, 317, 410, 648,\n",
            "       358, 258, 282, 376, 384, 224, 689, 593, 472, 347, 505, 715, 670,\n",
            "       653, 619, 612]), array([556, 577,  85, 242, 159, 524,  35, 540, 170, 621, 613, 682,  95,\n",
            "       563, 240, 574, 460, 553, 636, 206, 392, 397, 719, 217,   4, 647,\n",
            "       546,  98, 573, 406, 502,  47,  32, 200, 134,  27, 638, 230, 489,\n",
            "       378, 288, 418, 391, 592, 498, 138,  62, 471, 128, 706, 520,  64,\n",
            "        14, 156,  40, 492, 379, 187, 216,  52, 337, 295, 251, 461, 455,\n",
            "       724, 269, 201, 161, 555, 401, 476, 105, 565, 389,   1, 677, 561,\n",
            "        80, 205,  34, 508, 427, 454, 366,  91, 339, 564, 345, 241,  13,\n",
            "       315, 600, 387, 273, 166, 720, 646, 484, 709, 504, 243, 566, 562,\n",
            "       686, 189, 475, 681, 510,  58, 474, 560, 252,  21, 313, 459, 160,\n",
            "       276, 191, 385, 413, 491, 343, 308, 661, 130, 663,  99, 372,  87,\n",
            "       458, 330, 214, 466, 121, 614,  20, 700,  71, 106, 270, 435, 102])]\n"
          ]
        }
      ],
      "source": [
        "# Preparing the 5 folds\n",
        "k = 5\n",
        "N = len(numerical_labels)\n",
        "indexes = np.arange(N)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indexes)\n",
        "\n",
        "fold_size = int(np.ceil(N / k))\n",
        "\n",
        "folds = []\n",
        "for i in range(k):\n",
        "    start = i * fold_size\n",
        "    end = min(start + fold_size, N)\n",
        "    folds.append(indexes[start:end])\n",
        "\n",
        "print(folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCF9f-Il-Bv4",
        "outputId": "ed9d93c9-a6c5-4c08-c58f-d1ed1278e46a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing k-fold cross-validation for the Angle Usefulness gru ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:29<00:00,  2.62s/epoch, loss=0.1861, binary_accuracy=0.9259, precision=0.8677, recall=0.8081, val_loss=0.3080, val_binary_accuracy=0.8637, val_precision=0.7800, val_recall=0.6649]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 23ms/step - loss: 0.2635 - binary_accuracy: 0.8910 - precision: 0.8116 - recall: 0.7382\n",
            "Testing on Fold #2/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:33<00:00,  2.67s/epoch, loss=0.1687, binary_accuracy=0.9310, precision=0.8851, recall=0.8125, val_loss=0.2144, val_binary_accuracy=0.9049, val_precision=0.8485, val_recall=0.7491]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 24ms/step - loss: 0.2917 - binary_accuracy: 0.8733 - precision: 0.8277 - recall: 0.6590\n",
            "Testing on Fold #3/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:36<00:00,  2.70s/epoch, loss=0.1936, binary_accuracy=0.9203, precision=0.8635, recall=0.7857, val_loss=0.3059, val_binary_accuracy=0.8714, val_precision=0.7882, val_recall=0.6955]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 19ms/step - loss: 0.2814 - binary_accuracy: 0.8706 - precision: 0.8270 - recall: 0.6570\n",
            "Testing on Fold #4/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:07<00:00,  2.34s/epoch, loss=0.1773, binary_accuracy=0.9279, precision=0.8709, recall=0.8142, val_loss=0.3015, val_binary_accuracy=0.8804, val_precision=0.8246, val_recall=0.6980]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 28ms/step - loss: 0.3344 - binary_accuracy: 0.8560 - precision: 0.7953 - recall: 0.6373\n",
            "Testing on Fold #5/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:25<00:00,  2.57s/epoch, loss=0.1999, binary_accuracy=0.9162, precision=0.8642, recall=0.7634, val_loss=0.2825, val_binary_accuracy=0.8800, val_precision=0.8077, val_recall=0.6835]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 20ms/step - loss: 0.2804 - binary_accuracy: 0.8750 - precision: 0.8053 - recall: 0.6803\n",
            "Performing k-fold cross-validation for the Angle Usefulness lstm ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:48<00:00,  2.86s/epoch, loss=0.1737, binary_accuracy=0.9284, precision=0.8776, recall=0.8082, val_loss=0.2578, val_binary_accuracy=0.8909, val_precision=0.8425, val_recall=0.7153]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 21ms/step - loss: 0.2751 - binary_accuracy: 0.8648 - precision: 0.7587 - recall: 0.6788\n",
            "Testing on Fold #2/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:37<00:00,  2.71s/epoch, loss=0.1907, binary_accuracy=0.9211, precision=0.8696, recall=0.7819, val_loss=0.2718, val_binary_accuracy=0.8872, val_precision=0.8100, val_recall=0.7106]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 30ms/step - loss: 0.3071 - binary_accuracy: 0.8706 - precision: 0.8364 - recall: 0.6359\n",
            "Testing on Fold #3/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:48<00:00,  2.85s/epoch, loss=0.1675, binary_accuracy=0.9335, precision=0.8898, recall=0.8188, val_loss=0.2500, val_binary_accuracy=0.8868, val_precision=0.8130, val_recall=0.7370]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 28ms/step - loss: 0.2557 - binary_accuracy: 0.8804 - precision: 0.8276 - recall: 0.7024\n",
            "Testing on Fold #4/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:35<00:00,  2.69s/epoch, loss=0.1655, binary_accuracy=0.9346, precision=0.8887, recall=0.8246, val_loss=0.2691, val_binary_accuracy=0.8818, val_precision=0.8060, val_recall=0.7304]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 32ms/step - loss: 0.3443 - binary_accuracy: 0.8563 - precision: 0.8111 - recall: 0.6186\n",
            "Testing on Fold #5/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:51<00:00,  2.90s/epoch, loss=0.1747, binary_accuracy=0.9303, precision=0.8904, recall=0.8027, val_loss=0.2725, val_binary_accuracy=0.9072, val_precision=0.8494, val_recall=0.7649]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 32ms/step - loss: 0.2854 - binary_accuracy: 0.8801 - precision: 0.7903 - recall: 0.7289\n",
            "Performing k-fold cross-validation for the Angle Usefulness transformer ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [06:07<00:00,  4.59s/epoch, loss=0.1516, binary_accuracy=0.9403, precision=0.8859, recall=0.8566, val_loss=0.2808, val_binary_accuracy=0.8931, val_precision=0.8307, val_recall=0.7413]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 42ms/step - loss: 0.2443 - binary_accuracy: 0.9005 - precision: 0.8265 - recall: 0.7652\n",
            "Testing on Fold #2/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [06:16<00:00,  4.70s/epoch, loss=0.1486, binary_accuracy=0.9384, precision=0.8872, recall=0.8459, val_loss=0.2638, val_binary_accuracy=0.8986, val_precision=0.8061, val_recall=0.7766]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 39ms/step - loss: 0.3113 - binary_accuracy: 0.8845 - precision: 0.8152 - recall: 0.7295\n",
            "Testing on Fold #3/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [05:40<00:00,  4.26s/epoch, loss=0.1529, binary_accuracy=0.9371, precision=0.8880, recall=0.8384, val_loss=0.2348, val_binary_accuracy=0.9126, val_precision=0.8571, val_recall=0.7993]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 39ms/step - loss: 0.2330 - binary_accuracy: 0.9046 - precision: 0.8546 - recall: 0.7781\n",
            "Testing on Fold #4/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [05:30<00:00,  4.14s/epoch, loss=0.1325, binary_accuracy=0.9487, precision=0.9054, recall=0.8730, val_loss=0.2709, val_binary_accuracy=0.8976, val_precision=0.8371, val_recall=0.7628]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 44ms/step - loss: 0.2894 - binary_accuracy: 0.8801 - precision: 0.8096 - recall: 0.7342\n",
            "Testing on Fold #5/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [06:15<00:00,  4.69s/epoch, loss=0.1385, binary_accuracy=0.9465, precision=0.9051, recall=0.8627, val_loss=0.2062, val_binary_accuracy=0.9108, val_precision=0.8490, val_recall=0.7830]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 40ms/step - loss: 0.2510 - binary_accuracy: 0.8947 - precision: 0.7915 - recall: 0.8039\n"
          ]
        }
      ],
      "source": [
        "for seq_module in ['gru', 'lstm', 'transformer']:\n",
        "    print(f'Performing k-fold cross-validation for the Angle Usefulness {seq_module} ...')\n",
        "    for test in range(k):\n",
        "        print(f'Testing on Fold #{test+1}/{k} ...')\n",
        "        X_train, X_test, y_train, y_test = split_by_fold(\n",
        "            processed_sequences,\n",
        "            numerical_labels,\n",
        "            folds,\n",
        "            test\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        y_train = np.array(y_train, dtype=np.int32)\n",
        "        y_test = np.array(y_test, dtype=np.int32)\n",
        "        y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "        Y_train = np.empty((len(X_train), len(X_train[0][0])), np.float32)\n",
        "        Y_val = np.empty((len(X_val), len(X_train[0][0])), np.float32)\n",
        "        Y_test = np.empty((len(X_test), len(X_train[0][0])), np.float32)\n",
        "\n",
        "        for (Y, y) in [(Y_train, y_train), (Y_val, y_val), (Y_test, y_test)]:\n",
        "            for i in range(Y.shape[0]):\n",
        "                for angle in range(Y.shape[1]):\n",
        "                    if angle in label_angles[y[i]]:\n",
        "                        Y[i, angle] = 1\n",
        "                    else:\n",
        "                        Y[i, angle] = 0\n",
        "\n",
        "        usefulness_model = angle_usefulness_model(maxlen=MAXLEN, module=seq_module, lr=1e-3)\n",
        "\n",
        "        pbar_callback = TQDMProgressBar(epochs=80)\n",
        "\n",
        "        callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            MODEL_SAVE_PATH_USEFULNESS,\n",
        "            monitor=\"val_binary_accuracy\",\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode=\"max\",\n",
        "            save_freq=\"epoch\",\n",
        "            initial_value_threshold=None,\n",
        "        )\n",
        "\n",
        "        train_ds = UsefulnessDataset(X_train, Y_train, y_train, batch_size=batch_size, maxlen=MAXLEN, train=True)\n",
        "        val_ds = UsefulnessDataset(X_val, Y_val, y_val, batch_size=min(batch_size, len(X_val)), maxlen=MAXLEN, train=False)\n",
        "\n",
        "        history = usefulness_model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            batch_size=batch_size,\n",
        "            epochs=80,\n",
        "            callbacks=[callback, pbar_callback],\n",
        "            verbose=0, # Change this to 1 if you want progressive display while training\n",
        "        )\n",
        "\n",
        "        # Load the trained classifier model\n",
        "        usefulness_model = tf.keras.models.load_model(\n",
        "            MODEL_SAVE_PATH_USEFULNESS\n",
        "        )\n",
        "\n",
        "        test_ds = UsefulnessDataset(X_test, Y_test, y_test, batch_size=batch_size, maxlen=MAXLEN, train=False)\n",
        "        usefulness_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "m8tPsAjN_JAe",
        "outputId": "49c2691e-a9dc-4f50-e75d-55fa34e241e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing k-fold cross-validation for the Siamese gru ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 60/60 [07:16<00:00,  7.27s/epoch, Precision=0.8811, Recall=0.8101, binary_accuracy=0.9298, loss=0.1736, val_Precision=0.8323, val_Recall=0.7386, val_binary_accuracy=0.9012, val_loss=0.2754]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The `function` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1696127309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         similarity_model = tf.keras.models.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mMODEL_SAVE_PATH_SIMILARITY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    368\u001b[0m             )\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    525\u001b[0m             )\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects, safe_mode)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mfn_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__lambda__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         ):\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_for_lambda_deserialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minner_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             fn = python_utils.func_load(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36m_raise_for_lambda_deserialization\u001b[0;34m(arg_name, safe_mode)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raise_for_lambda_deserialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0;34mf\"The `{arg_name}` of this `Lambda` layer is a Python lambda. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;34m\"Deserializing it is unsafe. If you trust the source of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `function` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization()."
          ]
        }
      ],
      "source": [
        "for seq_module in ['gru', 'lstm', 'transformer']:\n",
        "    print(f'Performing k-fold cross-validation for the Siamese {seq_module} ...')\n",
        "    for test in range(k):\n",
        "        print(f'Testing on Fold #{test+1}/{k} ...')\n",
        "        X_train, X_test, y_train, y_test = split_by_fold(\n",
        "            processed_sequences,\n",
        "            numerical_labels,\n",
        "            folds,\n",
        "            test\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        y_train = np.array(y_train, dtype=np.int32)\n",
        "        y_test = np.array(y_test, dtype=np.int32)\n",
        "        y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "        train_batches = 10 * len(X_train) // batch_size\n",
        "        val_batches = 10 * len(X_val) // batch_size\n",
        "        train_ds = SimilarityDataset(X_train, y_train, num_batches=train_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=True)\n",
        "        val_ds = SimilarityDataset(X_val, y_val, num_batches=val_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "\n",
        "        callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            MODEL_SAVE_PATH_SIMILARITY,\n",
        "            monitor=\"val_binary_accuracy\",\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode=\"max\",\n",
        "            save_freq=\"epoch\",\n",
        "            initial_value_threshold=None,\n",
        "        )\n",
        "\n",
        "        pbar_callback = TQDMProgressBar(epochs=60)\n",
        "\n",
        "        # Build and train Siamese model\n",
        "        similarity_model = angle_similarity_model(maxlen=MAXLEN, module=seq_module, lr=1e-3)\n",
        "        history = similarity_model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=60,\n",
        "            callbacks=[callback, pbar_callback],\n",
        "            verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "        )\n",
        "\n",
        "        similarity_model = tf.keras.models.load_model(\n",
        "            MODEL_SAVE_PATH_SIMILARITY,\n",
        "            safe_mode=False\n",
        "        )\n",
        "\n",
        "        # Make a large dataset in terms of pair in order to have a more reliable test\n",
        "        # since we can't test on all possible pairs (very huge number)\n",
        "        test_batches = 1000 * len(X_test) // batch_size\n",
        "        # Evaluate the Siamese model on the test set\n",
        "        test_ds = SimilarityDataset(X_test, y_test, num_batches=test_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "        similarity_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ncJ_OCo_1ej"
      },
      "source": [
        "# One-shot Generalization Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3UuYgIW_0kd"
      },
      "outputs": [],
      "source": [
        "tests = [[3, 4, 13, 14, 15, 17], [1, 3, 4, 13, 14, 20], [1, 3, 6, 13, 19, 20], [1, 3, 7, 13, 14, 16], [1, 3, 5, 13, 14, 19], [1, 2, 3, 13, 19, 21], [0, 3, 6, 7, 13, 19], [1, 3, 4, 13, 14, 21], [3, 7, 13, 14, 16, 17], [1, 3, 10, 13, 20, 21]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJar4K7_ADxQ"
      },
      "outputs": [],
      "source": [
        "# Function to split data by specifying test labels\n",
        "def split_by_labels(data_x, data_y, test_labels):\n",
        "    X_train, X_test, y_train, y_test = [], [], [], []\n",
        "    for sample_x, sample_y in zip(data_x, data_y):\n",
        "        if sample_y in test_labels:\n",
        "            X_test.append(sample_x)\n",
        "            y_test.append(sample_y)\n",
        "        else:\n",
        "            X_train.append(sample_x)\n",
        "            y_train.append(sample_y)\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwMU3IQgAq5-"
      },
      "outputs": [],
      "source": [
        "print('Performing one-shot validation for the Angle Usefulness Transformer ...')\n",
        "for test in tests:\n",
        "    print(f'Testing on Exercises: {test} ...')\n",
        "    X_train, X_test, y_train, y_test = split_by_labels(\n",
        "        processed_sequences,\n",
        "        numerical_labels,\n",
        "        test_labels=test,\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    y_train = np.array(y_train, dtype=np.int32)\n",
        "    y_test = np.array(y_test, dtype=np.int32)\n",
        "    y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "    Y_train = np.empty((len(X_train), len(X_train[0][0])), np.float32)\n",
        "    Y_val = np.empty((len(X_val), len(X_train[0][0])), np.float32)\n",
        "    Y_test = np.empty((len(X_test), len(X_train[0][0])), np.float32)\n",
        "\n",
        "    for (Y, y) in [(Y_train, y_train), (Y_val, y_val), (Y_test, y_test)]:\n",
        "        for i in range(Y.shape[0]):\n",
        "            for angle in range(Y.shape[1]):\n",
        "                if angle in label_angles[y[i]]:\n",
        "                    Y[i, angle] = 1\n",
        "                else:\n",
        "                    Y[i, angle] = 0\n",
        "\n",
        "    usefulness_model = angle_usefulness_model(maxlen=MAXLEN, module='transformer', lr=1e-4)\n",
        "\n",
        "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        MODEL_SAVE_PATH_USEFULNESS,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode=\"max\",\n",
        "        save_freq=\"epoch\",\n",
        "        initial_value_threshold=None,\n",
        "    )\n",
        "\n",
        "    pbar_callback = TQDMProgressBar(epochs=80)\n",
        "\n",
        "    train_ds = UsefulnessDataset(X_train, Y_train, y_train, batch_size=batch_size, maxlen=MAXLEN, train=True)\n",
        "    val_ds = UsefulnessDataset(X_val, Y_val, y_val, batch_size=min(batch_size, len(X_val)), maxlen=MAXLEN, train=False)\n",
        "\n",
        "    history = usefulness_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        batch_size=batch_size,\n",
        "        epochs=80,\n",
        "        callbacks=[callback, pbar_callback],\n",
        "        verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "    )\n",
        "\n",
        "    # Load the trained classifier model\n",
        "    usefulness_model = tf.keras.models.load_model(\n",
        "        MODEL_SAVE_PATH_USEFULNESS\n",
        "    )\n",
        "\n",
        "    test_ds = UsefulnessDataset(X_test, Y_test, y_test, batch_size=batch_size, maxlen=MAXLEN, train=False)\n",
        "    usefulness_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFhxjwVJARTw"
      },
      "outputs": [],
      "source": [
        "print('Performing one-shot validation for the Siamese Transformer ...')\n",
        "for test in tests:\n",
        "    print(f'Testing on Exercises: {test} ...')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = split_by_labels(\n",
        "        processed_sequences,\n",
        "        numerical_labels,\n",
        "        test_labels=test,\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_batches = 10 * len(X_train) // batch_size\n",
        "    val_batches = 10 * len(X_val) // batch_size\n",
        "    train_ds = SimilarityDataset(X_train, y_train, num_batches=train_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=True)\n",
        "    val_ds = SimilarityDataset(X_val, y_val, num_batches=val_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "\n",
        "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        MODEL_SAVE_PATH_SIMILARITY,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode=\"max\",\n",
        "        save_freq=\"epoch\",\n",
        "        initial_value_threshold=None,\n",
        "    )\n",
        "\n",
        "    pbar_callback = TQDMProgressBar(epochs=60)\n",
        "\n",
        "    # Build and train Siamese model\n",
        "    similarity_model = angle_similarity_model(maxlen=MAXLEN, module='transformer', lr=1e-3)\n",
        "    history = similarity_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=60,\n",
        "        callbacks=[callback, pbar_callback],\n",
        "        verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "    )\n",
        "\n",
        "    # Load the trained Siamese model\n",
        "    similarity_model = tf.keras.models.load_model(\n",
        "        MODEL_SAVE_PATH_SIMILARITY,\n",
        "    )\n",
        "\n",
        "    # Make a large dataset in terms of pair in order to have a more reliable test\n",
        "    # since we can't test on all possible pairs (very huge number)\n",
        "    test_batches = 1000 * len(X_test) // batch_size\n",
        "    # Evaluate the Siamese model on the test set\n",
        "    test_ds = SimilarityDataset(X_test, y_test, num_batches=test_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "    similarity_model.evaluate(test_ds, verbose=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}