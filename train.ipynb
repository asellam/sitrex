{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_HAe5VzbUd7"
      },
      "source": [
        "# Get the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eA7POIpDbUd8",
        "outputId": "3973b48b-f791-408b-d678-c93e6a41af41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sitrex' already exists and is not an empty directory.\n",
            "/content/sitrex\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/asellam/sitrex.git\n",
        "%cd sitrex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QTceCwapbUd_",
        "outputId": "80274c4c-308a-4eba-a3dd-f4f3a07ac403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.23 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.15.3)\n",
            "Requirement already satisfied: tqdm>=4.65 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (1.6.1)\n",
            "Requirement already satisfied: kagglehub>=0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.3.12)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.74.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (3.14.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (4.25.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (4.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (1.14.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12->-r requirements.txt (line 2)) (0.37.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 12)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 12)) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 15)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 15)) (2.32.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12->-r requirements.txt (line 2)) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2)) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 15)) (2025.8.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12->-r requirements.txt (line 2)) (3.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6fubIm76ZdW"
      },
      "source": [
        "# Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZZt0APPizmv5"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download('abdellah213/sitrex-dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbuBNCCw69KI"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p0IEFaNKzz3F"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"./sitrex\")\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sitrex.preprocessing import load_dataset, preprocess_data, SimilarityDataset, UsefulnessDataset\n",
        "from sitrex.model import angle_usefulness_model, angle_similarity_model, TQDMProgressBar\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "# Define constants\n",
        "DATASET_PATH = os.path.join(path, 'dataset')\n",
        "# EarlyStop save path for the Siamese Transformer\n",
        "MODEL_SAVE_PATH_SIMILARITY = './angle_similarity_model.keras'\n",
        "# EarlyStop save path for the Angle Usefulness Transformer\n",
        "MODEL_SAVE_PATH_USEFULNESS = './angle_usefulness_model.keras'\n",
        "# Maximum sequence length (L)\n",
        "MAXLEN = 100\n",
        "# Batch Size\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELxuOym7bUeA"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMNQw9Ql7_YW",
        "outputId": "2e10acf0-18b6-47d8-c9a9-14557d4b70e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset:  33%|███▎      | 242/727 [01:10<05:08,  1.57it/s]"
          ]
        }
      ],
      "source": [
        "all_sequences, labels, exercise_angles = load_dataset(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Fvh4Ak7Hnw"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3XNnuaE8P0H"
      },
      "outputs": [],
      "source": [
        "processed_sequences, numerical_labels, label_angles = preprocess_data(all_sequences, labels, exercise_angles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1-x8hJi8Wxz"
      },
      "source": [
        "# Tensorflow Dataset and Model Routines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfoMcUK9ceH"
      },
      "source": [
        "# K-Fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45B5HZpO9Vl9"
      },
      "outputs": [],
      "source": [
        "def split_by_fold(data_x, data_y, folds, test):\n",
        "    train_x, test_x, train_y, test_y = [], [], [], []\n",
        "    for fold, split in enumerate(folds):\n",
        "        for sample in split:\n",
        "            if fold == test:\n",
        "                test_x.append(data_x[sample])\n",
        "                test_y.append(data_y[sample])\n",
        "            else:\n",
        "                train_x.append(data_x[sample])\n",
        "                train_y.append(data_y[sample])\n",
        "    return train_x, test_x, train_y, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOFTlTJZ9wRL"
      },
      "outputs": [],
      "source": [
        "# Preparing the 5 folds\n",
        "k = 5\n",
        "N = len(numerical_labels)\n",
        "indexes = np.arange(N)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indexes)\n",
        "\n",
        "fold_size = int(np.ceil(N / k))\n",
        "\n",
        "folds = []\n",
        "for i in range(k):\n",
        "    start = i * fold_size\n",
        "    end = min(start + fold_size, N)\n",
        "    folds.append(indexes[start:end])\n",
        "\n",
        "print(folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCF9f-Il-Bv4"
      },
      "outputs": [],
      "source": [
        "for seq_module in ['gru', 'lstm', 'transformer']:\n",
        "    print(f'Performing k-fold cross-validation for the Angle Usefulness {seq_module} ...')\n",
        "    for test in range(k):\n",
        "        print(f'Testing on Fold #{test+1}/{k} ...')\n",
        "        X_train, X_test, y_train, y_test = split_by_fold(\n",
        "            processed_sequences,\n",
        "            numerical_labels,\n",
        "            folds,\n",
        "            test\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        y_train = np.array(y_train, dtype=np.int32)\n",
        "        y_test = np.array(y_test, dtype=np.int32)\n",
        "        y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "        Y_train = np.empty((len(X_train), len(X_train[0][0])), np.float32)\n",
        "        Y_val = np.empty((len(X_val), len(X_train[0][0])), np.float32)\n",
        "        Y_test = np.empty((len(X_test), len(X_train[0][0])), np.float32)\n",
        "\n",
        "        for (Y, y) in [(Y_train, y_train), (Y_val, y_val), (Y_test, y_test)]:\n",
        "            for i in range(Y.shape[0]):\n",
        "                for angle in range(Y.shape[1]):\n",
        "                    if angle in label_angles[y[i]]:\n",
        "                        Y[i, angle] = 1\n",
        "                    else:\n",
        "                        Y[i, angle] = 0\n",
        "\n",
        "        usefulness_model = angle_usefulness_model(maxlen=MAXLEN, module=seq_module, lr=1e-3)\n",
        "\n",
        "        pbar_callback = TQDMProgressBar(epochs=80)\n",
        "\n",
        "        callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            MODEL_SAVE_PATH_USEFULNESS,\n",
        "            monitor=\"val_binary_accuracy\",\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode=\"max\",\n",
        "            save_freq=\"epoch\",\n",
        "            initial_value_threshold=None,\n",
        "        )\n",
        "\n",
        "        train_ds = UsefulnessDataset(X_train, Y_train, y_train, batch_size=batch_size, maxlen=MAXLEN, train=True)\n",
        "        val_ds = UsefulnessDataset(X_val, Y_val, y_val, batch_size=min(batch_size, len(X_val)), maxlen=MAXLEN, train=False)\n",
        "\n",
        "        history = usefulness_model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            batch_size=batch_size,\n",
        "            epochs=80,\n",
        "            callbacks=[callback, pbar_callback],\n",
        "            verbose=0, # Change this to 1 if you want progressive display while training\n",
        "        )\n",
        "\n",
        "        # Load the trained classifier model\n",
        "        usefulness_model = tf.keras.models.load_model(\n",
        "            MODEL_SAVE_PATH_USEFULNESS\n",
        "        )\n",
        "\n",
        "        test_ds = UsefulnessDataset(X_test, Y_test, y_test, batch_size=batch_size, maxlen=MAXLEN, train=False)\n",
        "        usefulness_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8tPsAjN_JAe"
      },
      "outputs": [],
      "source": [
        "for seq_module in ['gru', 'lstm', 'transformer']:\n",
        "    print(f'Performing k-fold cross-validation for the Siamese {seq_module} ...')\n",
        "    for test in range(k):\n",
        "        print(f'Testing on Fold #{test+1}/{k} ...')\n",
        "        X_train, X_test, y_train, y_test = split_by_fold(\n",
        "            processed_sequences,\n",
        "            numerical_labels,\n",
        "            folds,\n",
        "            test\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        y_train = np.array(y_train, dtype=np.int32)\n",
        "        y_test = np.array(y_test, dtype=np.int32)\n",
        "        y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "        train_batches = 10 * len(X_train) // batch_size\n",
        "        val_batches = 10 * len(X_val) // batch_size\n",
        "        train_ds = SimilarityDataset(X_train, y_train, num_batches=train_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=True)\n",
        "        val_ds = SimilarityDataset(X_val, y_val, num_batches=val_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "\n",
        "        callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            MODEL_SAVE_PATH_SIMILARITY,\n",
        "            monitor=\"val_binary_accuracy\",\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode=\"max\",\n",
        "            save_freq=\"epoch\",\n",
        "            initial_value_threshold=None,\n",
        "        )\n",
        "\n",
        "        pbar_callback = TQDMProgressBar(epochs=60)\n",
        "\n",
        "        # Build and train Siamese model\n",
        "        similarity_model = angle_similarity_model(maxlen=MAXLEN, module=seq_module, lr=1e-3)\n",
        "        history = similarity_model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=60,\n",
        "            callbacks=[callback, pbar_callback],\n",
        "            verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "        )\n",
        "\n",
        "        similarity_model = tf.keras.models.load_model(\n",
        "            MODEL_SAVE_PATH_SIMILARITY,\n",
        "        )\n",
        "\n",
        "        # Make a large dataset in terms of pair in order to have a more reliable test\n",
        "        # since we can't test on all possible pairs (very huge number)\n",
        "        test_batches = 1000 * len(X_test) // batch_size\n",
        "        # Evaluate the Siamese model on the test set\n",
        "        test_ds = SimilarityDataset(X_test, y_test, num_batches=test_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "        similarity_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ncJ_OCo_1ej"
      },
      "source": [
        "# One-shot Generalization Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3UuYgIW_0kd"
      },
      "outputs": [],
      "source": [
        "tests = [[3, 4, 13, 14, 15, 17], [1, 3, 4, 13, 14, 20], [1, 3, 6, 13, 19, 20], [1, 3, 7, 13, 14, 16], [1, 3, 5, 13, 14, 19], [1, 2, 3, 13, 19, 21], [0, 3, 6, 7, 13, 19], [1, 3, 4, 13, 14, 21], [3, 7, 13, 14, 16, 17], [1, 3, 10, 13, 20, 21]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJar4K7_ADxQ"
      },
      "outputs": [],
      "source": [
        "# Function to split data by specifying test labels\n",
        "def split_by_labels(data_x, data_y, test_labels):\n",
        "    X_train, X_test, y_train, y_test = [], [], [], []\n",
        "    for sample_x, sample_y in zip(data_x, data_y):\n",
        "        if sample_y in test_labels:\n",
        "            X_test.append(sample_x)\n",
        "            y_test.append(sample_y)\n",
        "        else:\n",
        "            X_train.append(sample_x)\n",
        "            y_train.append(sample_y)\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwMU3IQgAq5-"
      },
      "outputs": [],
      "source": [
        "print('Performing one-shot validation for the Angle Usefulness Transformer ...')\n",
        "for test in tests:\n",
        "    print(f'Testing on Exercises: {test} ...')\n",
        "    X_train, X_test, y_train, y_test = split_by_labels(\n",
        "        processed_sequences,\n",
        "        numerical_labels,\n",
        "        test_labels=test,\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    y_train = np.array(y_train, dtype=np.int32)\n",
        "    y_test = np.array(y_test, dtype=np.int32)\n",
        "    y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "    Y_train = np.empty((len(X_train), len(X_train[0][0])), np.float32)\n",
        "    Y_val = np.empty((len(X_val), len(X_train[0][0])), np.float32)\n",
        "    Y_test = np.empty((len(X_test), len(X_train[0][0])), np.float32)\n",
        "\n",
        "    for (Y, y) in [(Y_train, y_train), (Y_val, y_val), (Y_test, y_test)]:\n",
        "        for i in range(Y.shape[0]):\n",
        "            for angle in range(Y.shape[1]):\n",
        "                if angle in label_angles[y[i]]:\n",
        "                    Y[i, angle] = 1\n",
        "                else:\n",
        "                    Y[i, angle] = 0\n",
        "\n",
        "    usefulness_model = angle_usefulness_model(maxlen=MAXLEN, module='transformer', lr=1e-4)\n",
        "\n",
        "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        MODEL_SAVE_PATH_USEFULNESS,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode=\"max\",\n",
        "        save_freq=\"epoch\",\n",
        "        initial_value_threshold=None,\n",
        "    )\n",
        "\n",
        "    pbar_callback = TQDMProgressBar(epochs=80)\n",
        "\n",
        "    train_ds = UsefulnessDataset(X_train, Y_train, y_train, batch_size=batch_size, maxlen=MAXLEN, train=True)\n",
        "    val_ds = UsefulnessDataset(X_val, Y_val, y_val, batch_size=min(batch_size, len(X_val)), maxlen=MAXLEN, train=False)\n",
        "\n",
        "    history = usefulness_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        batch_size=batch_size,\n",
        "        epochs=80,\n",
        "        callbacks=[callback, pbar_callback],\n",
        "        verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "    )\n",
        "\n",
        "    # Load the trained classifier model\n",
        "    usefulness_model = tf.keras.models.load_model(\n",
        "        MODEL_SAVE_PATH_USEFULNESS\n",
        "    )\n",
        "\n",
        "    test_ds = UsefulnessDataset(X_test, Y_test, y_test, batch_size=batch_size, maxlen=MAXLEN, train=False)\n",
        "    usefulness_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFhxjwVJARTw"
      },
      "outputs": [],
      "source": [
        "print('Performing one-shot validation for the Siamese Transformer ...')\n",
        "for test in tests:\n",
        "    print(f'Testing on Exercises: {test} ...')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = split_by_labels(\n",
        "        processed_sequences,\n",
        "        numerical_labels,\n",
        "        test_labels=test,\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_batches = 10 * len(X_train) // batch_size\n",
        "    val_batches = 10 * len(X_val) // batch_size\n",
        "    train_ds = SimilarityDataset(X_train, y_train, num_batches=train_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=True)\n",
        "    val_ds = SimilarityDataset(X_val, y_val, num_batches=val_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "\n",
        "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        MODEL_SAVE_PATH_SIMILARITY,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode=\"max\",\n",
        "        save_freq=\"epoch\",\n",
        "        initial_value_threshold=None,\n",
        "    )\n",
        "\n",
        "    pbar_callback = TQDMProgressBar(epochs=60)\n",
        "\n",
        "    # Build and train Siamese model\n",
        "    similarity_model = angle_similarity_model(maxlen=MAXLEN, module='transformer', lr=1e-3)\n",
        "    history = similarity_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=60,\n",
        "        callbacks=[callback, pbar_callback],\n",
        "        verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "    )\n",
        "\n",
        "    # Load the trained Siamese model\n",
        "    similarity_model = tf.keras.models.load_model(\n",
        "        MODEL_SAVE_PATH_SIMILARITY,\n",
        "    )\n",
        "\n",
        "    # Make a large dataset in terms of pair in order to have a more reliable test\n",
        "    # since we can't test on all possible pairs (very huge number)\n",
        "    test_batches = 1000 * len(X_test) // batch_size\n",
        "    # Evaluate the Siamese model on the test set\n",
        "    test_ds = SimilarityDataset(X_test, y_test, num_batches=test_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "    similarity_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50YHDjyqbUeE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}