{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_HAe5VzbUd7"
      },
      "source": [
        "# Get the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA7POIpDbUd8",
        "outputId": "5d8ad6bb-84e2-4f01-8f50-0e0915871c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sitrex' already exists and is not an empty directory.\n",
            "/content/sitrex\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/asellam/sitrex.git\n",
        "%cd sitrex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QTceCwapbUd_",
        "outputId": "cea40309-b3d8-49ab-fa1e-bcc13c1ece1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2.0,>=1.23 (from -r requirements.txt (line 2))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.16.1)\n",
            "Requirement already satisfied: tqdm>=4.65 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
            "Requirement already satisfied: kagglehub>=0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (0.3.12)\n",
            "Collecting mediapipe==0.10.21 (from -r requirements.txt (line 15))\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe==0.10.21->-r requirements.txt (line 15))\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.21->-r requirements.txt (line 15))\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.21->-r requirements.txt (line 15)) (0.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.2->-r requirements.txt (line 9)) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 12)) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 12)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub>=0.2->-r requirements.txt (line 12)) (2.32.4)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21->-r requirements.txt (line 15)) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.21->-r requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe==0.10.21->-r requirements.txt (line 15))\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub>=0.2->-r requirements.txt (line 12)) (2025.8.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.21->-r requirements.txt (line 15)) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.21->-r requirements.txt (line 15)) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "0d90ab6d48a648e5b4580b9e9760110f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6fubIm76ZdW"
      },
      "source": [
        "# Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZZt0APPizmv5"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download('abdellah213/sitrex-dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbuBNCCw69KI"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p0IEFaNKzz3F"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sitrex.preprocessing import load_dataset, preprocess_data, SimilarityDataset, UsefulnessDataset\n",
        "from sitrex.model import angle_usefulness_model, angle_similarity_model, TQDMProgressBar\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "# Define constants\n",
        "DATASET_PATH = os.path.join(path, 'dataset')\n",
        "# EarlyStop save path for the Siamese Transformer\n",
        "MODEL_SAVE_PATH_SIMILARITY = './angle_similarity_model.keras'\n",
        "# EarlyStop save path for the Angle Usefulness Transformer\n",
        "MODEL_SAVE_PATH_USEFULNESS = './angle_usefulness_model.keras'\n",
        "# Maximum sequence length (L)\n",
        "MAXLEN = 100\n",
        "# Batch Size\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELxuOym7bUeA"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMNQw9Ql7_YW",
        "outputId": "2c2e3549-7481-4349-c38c-a9f0e3a58433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset:  65%|██████▌   | 476/727 [02:24<01:59,  2.10it/s]"
          ]
        }
      ],
      "source": [
        "all_sequences, labels, exercise_angles = load_dataset(DATASET_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Fvh4Ak7Hnw"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3XNnuaE8P0H"
      },
      "outputs": [],
      "source": [
        "processed_sequences, numerical_labels, label_angles = preprocess_data(all_sequences, labels, exercise_angles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1-x8hJi8Wxz"
      },
      "source": [
        "# Tensorflow Dataset and Model Routines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAfoMcUK9ceH"
      },
      "source": [
        "# K-Fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45B5HZpO9Vl9"
      },
      "outputs": [],
      "source": [
        "def split_by_fold(data_x, data_y, folds, test):\n",
        "    train_x, test_x, train_y, test_y = [], [], [], []\n",
        "    for fold, split in enumerate(folds):\n",
        "        for sample in split:\n",
        "            if fold == test:\n",
        "                test_x.append(data_x[sample])\n",
        "                test_y.append(data_y[sample])\n",
        "            else:\n",
        "                train_x.append(data_x[sample])\n",
        "                train_y.append(data_y[sample])\n",
        "    return train_x, test_x, train_y, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOFTlTJZ9wRL"
      },
      "outputs": [],
      "source": [
        "# Preparing the 5 folds\n",
        "k = 5\n",
        "N = len(numerical_labels)\n",
        "indexes = np.arange(N)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indexes)\n",
        "\n",
        "fold_size = int(np.ceil(N / k))\n",
        "\n",
        "folds = []\n",
        "for i in range(k):\n",
        "    start = i * fold_size\n",
        "    end = min(start + fold_size, N)\n",
        "    folds.append(indexes[start:end])\n",
        "\n",
        "print(folds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCF9f-Il-Bv4",
        "outputId": "ed9d93c9-a6c5-4c08-c58f-d1ed1278e46a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing k-fold cross-validation for the Angle Usefulness gru ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:29<00:00,  2.62s/epoch, loss=0.1861, binary_accuracy=0.9259, precision=0.8677, recall=0.8081, val_loss=0.3080, val_binary_accuracy=0.8637, val_precision=0.7800, val_recall=0.6649]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 23ms/step - loss: 0.2635 - binary_accuracy: 0.8910 - precision: 0.8116 - recall: 0.7382\n",
            "Testing on Fold #2/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:33<00:00,  2.67s/epoch, loss=0.1687, binary_accuracy=0.9310, precision=0.8851, recall=0.8125, val_loss=0.2144, val_binary_accuracy=0.9049, val_precision=0.8485, val_recall=0.7491]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 24ms/step - loss: 0.2917 - binary_accuracy: 0.8733 - precision: 0.8277 - recall: 0.6590\n",
            "Testing on Fold #3/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:36<00:00,  2.70s/epoch, loss=0.1936, binary_accuracy=0.9203, precision=0.8635, recall=0.7857, val_loss=0.3059, val_binary_accuracy=0.8714, val_precision=0.7882, val_recall=0.6955]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 19ms/step - loss: 0.2814 - binary_accuracy: 0.8706 - precision: 0.8270 - recall: 0.6570\n",
            "Testing on Fold #4/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:07<00:00,  2.34s/epoch, loss=0.1773, binary_accuracy=0.9279, precision=0.8709, recall=0.8142, val_loss=0.3015, val_binary_accuracy=0.8804, val_precision=0.8246, val_recall=0.6980]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 28ms/step - loss: 0.3344 - binary_accuracy: 0.8560 - precision: 0.7953 - recall: 0.6373\n",
            "Testing on Fold #5/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:25<00:00,  2.57s/epoch, loss=0.1999, binary_accuracy=0.9162, precision=0.8642, recall=0.7634, val_loss=0.2825, val_binary_accuracy=0.8800, val_precision=0.8077, val_recall=0.6835]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 20ms/step - loss: 0.2804 - binary_accuracy: 0.8750 - precision: 0.8053 - recall: 0.6803\n",
            "Performing k-fold cross-validation for the Angle Usefulness lstm ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:48<00:00,  2.86s/epoch, loss=0.1737, binary_accuracy=0.9284, precision=0.8776, recall=0.8082, val_loss=0.2578, val_binary_accuracy=0.8909, val_precision=0.8425, val_recall=0.7153]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 21ms/step - loss: 0.2751 - binary_accuracy: 0.8648 - precision: 0.7587 - recall: 0.6788\n",
            "Testing on Fold #2/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:37<00:00,  2.71s/epoch, loss=0.1907, binary_accuracy=0.9211, precision=0.8696, recall=0.7819, val_loss=0.2718, val_binary_accuracy=0.8872, val_precision=0.8100, val_recall=0.7106]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 30ms/step - loss: 0.3071 - binary_accuracy: 0.8706 - precision: 0.8364 - recall: 0.6359\n",
            "Testing on Fold #3/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:48<00:00,  2.85s/epoch, loss=0.1675, binary_accuracy=0.9335, precision=0.8898, recall=0.8188, val_loss=0.2500, val_binary_accuracy=0.8868, val_precision=0.8130, val_recall=0.7370]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 28ms/step - loss: 0.2557 - binary_accuracy: 0.8804 - precision: 0.8276 - recall: 0.7024\n",
            "Testing on Fold #4/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:35<00:00,  2.69s/epoch, loss=0.1655, binary_accuracy=0.9346, precision=0.8887, recall=0.8246, val_loss=0.2691, val_binary_accuracy=0.8818, val_precision=0.8060, val_recall=0.7304]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 32ms/step - loss: 0.3443 - binary_accuracy: 0.8563 - precision: 0.8111 - recall: 0.6186\n",
            "Testing on Fold #5/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [03:51<00:00,  2.90s/epoch, loss=0.1747, binary_accuracy=0.9303, precision=0.8904, recall=0.8027, val_loss=0.2725, val_binary_accuracy=0.9072, val_precision=0.8494, val_recall=0.7649]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 32ms/step - loss: 0.2854 - binary_accuracy: 0.8801 - precision: 0.7903 - recall: 0.7289\n",
            "Performing k-fold cross-validation for the Angle Usefulness transformer ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [06:07<00:00,  4.59s/epoch, loss=0.1516, binary_accuracy=0.9403, precision=0.8859, recall=0.8566, val_loss=0.2808, val_binary_accuracy=0.8931, val_precision=0.8307, val_recall=0.7413]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 42ms/step - loss: 0.2443 - binary_accuracy: 0.9005 - precision: 0.8265 - recall: 0.7652\n",
            "Testing on Fold #2/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [06:16<00:00,  4.70s/epoch, loss=0.1486, binary_accuracy=0.9384, precision=0.8872, recall=0.8459, val_loss=0.2638, val_binary_accuracy=0.8986, val_precision=0.8061, val_recall=0.7766]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 39ms/step - loss: 0.3113 - binary_accuracy: 0.8845 - precision: 0.8152 - recall: 0.7295\n",
            "Testing on Fold #3/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [05:40<00:00,  4.26s/epoch, loss=0.1529, binary_accuracy=0.9371, precision=0.8880, recall=0.8384, val_loss=0.2348, val_binary_accuracy=0.9126, val_precision=0.8571, val_recall=0.7993]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 39ms/step - loss: 0.2330 - binary_accuracy: 0.9046 - precision: 0.8546 - recall: 0.7781\n",
            "Testing on Fold #4/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [05:30<00:00,  4.14s/epoch, loss=0.1325, binary_accuracy=0.9487, precision=0.9054, recall=0.8730, val_loss=0.2709, val_binary_accuracy=0.8976, val_precision=0.8371, val_recall=0.7628]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 1s 44ms/step - loss: 0.2894 - binary_accuracy: 0.8801 - precision: 0.8096 - recall: 0.7342\n",
            "Testing on Fold #5/5 ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 80/80 [06:15<00:00,  4.69s/epoch, loss=0.1385, binary_accuracy=0.9465, precision=0.9051, recall=0.8627, val_loss=0.2062, val_binary_accuracy=0.9108, val_precision=0.8490, val_recall=0.7830]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 40ms/step - loss: 0.2510 - binary_accuracy: 0.8947 - precision: 0.7915 - recall: 0.8039\n"
          ]
        }
      ],
      "source": [
        "epochs = 80\n",
        "for seq_module in ['gru', 'lstm', 'transformer']:\n",
        "    print(f'Performing k-fold cross-validation for the Angle Usefulness {seq_module} ...')\n",
        "    for test in range(k):\n",
        "        print(f'Testing on Fold #{test+1}/{k} ...')\n",
        "        X_train, X_test, y_train, y_test = split_by_fold(\n",
        "            processed_sequences,\n",
        "            numerical_labels,\n",
        "            folds,\n",
        "            test\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        y_train = np.array(y_train, dtype=np.int32)\n",
        "        y_test = np.array(y_test, dtype=np.int32)\n",
        "        y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "        Y_train = np.empty((len(X_train), len(X_train[0][0])), np.float32)\n",
        "        Y_val = np.empty((len(X_val), len(X_train[0][0])), np.float32)\n",
        "        Y_test = np.empty((len(X_test), len(X_train[0][0])), np.float32)\n",
        "\n",
        "        for (Y, y) in [(Y_train, y_train), (Y_val, y_val), (Y_test, y_test)]:\n",
        "            for i in range(Y.shape[0]):\n",
        "                for angle in range(Y.shape[1]):\n",
        "                    if angle in label_angles[y[i]]:\n",
        "                        Y[i, angle] = 1\n",
        "                    else:\n",
        "                        Y[i, angle] = 0\n",
        "\n",
        "        usefulness_model = angle_usefulness_model(maxlen=MAXLEN, module=seq_module, lr=1e-3)\n",
        "\n",
        "        pbar_callback = TQDMProgressBar(epochs=epochs)\n",
        "\n",
        "        callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            MODEL_SAVE_PATH_USEFULNESS,\n",
        "            monitor=\"val_binary_accuracy\",\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode=\"max\",\n",
        "            save_freq=\"epoch\",\n",
        "            initial_value_threshold=None,\n",
        "        )\n",
        "\n",
        "        train_ds = UsefulnessDataset(X_train, Y_train, y_train, batch_size=batch_size, maxlen=MAXLEN, train=True)\n",
        "        val_ds = UsefulnessDataset(X_val, Y_val, y_val, batch_size=min(batch_size, len(X_val)), maxlen=MAXLEN, train=False)\n",
        "\n",
        "        history = usefulness_model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            callbacks=[callback, pbar_callback],\n",
        "            verbose=0, # Change this to 1 if you want progressive display while training\n",
        "        )\n",
        "\n",
        "        # Load the trained classifier model\n",
        "        usefulness_model = tf.keras.models.load_model(\n",
        "            MODEL_SAVE_PATH_USEFULNESS\n",
        "        )\n",
        "\n",
        "        test_ds = UsefulnessDataset(X_test, Y_test, y_test, batch_size=batch_size, maxlen=MAXLEN, train=False)\n",
        "        usefulness_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "m8tPsAjN_JAe",
        "outputId": "3dad5c85-b6a0-498b-fc85-2780ece3fd81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing k-fold cross-validation for the Siamese gru ...\n",
            "Testing on Fold #1/5 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 60/60 [07:00<00:00,  7.01s/epoch, Precision=0.8834, Recall=0.8109, binary_accuracy=0.9301, loss=0.1735, val_Precision=0.8465, val_Recall=0.7550, val_binary_accuracy=0.9058, val_loss=0.2456]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=(['<KerasTensor shape=(None, 32), dtype=float32, sparse=False, ragged=False, name=keras_tensor_43>', '<KerasTensor shape=(None, 32), dtype=float32, sparse=False, ragged=False, name=keras_tensor_45>'],)\n  • kwargs={'mask': ['None', 'None']}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-132382095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         similarity_model = tf.keras.models.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mMODEL_SAVE_PATH_SIMILARITY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    368\u001b[0m             )\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0mnode_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_data_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                         \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                     \u001b[0;31m# If the node does not have all inbound layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# Call layer on its inputs, thus creating the node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# and building the layer if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 raise NotImplementedError(\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0;34m\"We could not automatically infer the shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;34m\"the Lambda's output. Please specify the `output_shape` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n\nArguments received by Lambda.call():\n  • args=(['<KerasTensor shape=(None, 32), dtype=float32, sparse=False, ragged=False, name=keras_tensor_43>', '<KerasTensor shape=(None, 32), dtype=float32, sparse=False, ragged=False, name=keras_tensor_45>'],)\n  • kwargs={'mask': ['None', 'None']}"
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "for seq_module in ['gru', 'lstm', 'transformer']:\n",
        "    print(f'Performing k-fold cross-validation for the Siamese {seq_module} ...')\n",
        "    for test in range(k):\n",
        "        print(f'Testing on Fold #{test+1}/{k} ...')\n",
        "        X_train, X_test, y_train, y_test = split_by_fold(\n",
        "            processed_sequences,\n",
        "            numerical_labels,\n",
        "            folds,\n",
        "            test\n",
        "        )\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        y_train = np.array(y_train, dtype=np.int32)\n",
        "        y_test = np.array(y_test, dtype=np.int32)\n",
        "        y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "        train_batches = 10 * len(X_train) // batch_size\n",
        "        val_batches = 10 * len(X_val) // batch_size\n",
        "        train_ds = SimilarityDataset(X_train, y_train, num_batches=train_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=True)\n",
        "        val_ds = SimilarityDataset(X_val, y_val, num_batches=val_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "\n",
        "        callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            MODEL_SAVE_PATH_SIMILARITY,\n",
        "            monitor=\"val_binary_accuracy\",\n",
        "            verbose=0,\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode=\"max\",\n",
        "            save_freq=\"epoch\",\n",
        "            initial_value_threshold=None,\n",
        "        )\n",
        "\n",
        "        pbar_callback = TQDMProgressBar(epochs=epochs)\n",
        "\n",
        "        # Build and train Siamese model\n",
        "        similarity_model = angle_similarity_model(maxlen=MAXLEN, module=seq_module, lr=1e-3)\n",
        "        history = similarity_model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=epochs,\n",
        "            callbacks=[callback, pbar_callback],\n",
        "            verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "        )\n",
        "\n",
        "        similarity_model = tf.keras.models.load_model(\n",
        "            MODEL_SAVE_PATH_SIMILARITY,\n",
        "            safe_mode=False\n",
        "        )\n",
        "\n",
        "        # Make a large dataset in terms of pair in order to have a more reliable test\n",
        "        # since we can't test on all possible pairs (very huge number)\n",
        "        test_batches = 1000 * len(X_test) // batch_size\n",
        "        # Evaluate the Siamese model on the test set\n",
        "        test_ds = SimilarityDataset(X_test, y_test, num_batches=test_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "        similarity_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ncJ_OCo_1ej"
      },
      "source": [
        "# One-shot Generalization Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3UuYgIW_0kd"
      },
      "outputs": [],
      "source": [
        "tests = [[3, 4, 13, 14, 15, 17], [1, 3, 4, 13, 14, 20], [1, 3, 6, 13, 19, 20], [1, 3, 7, 13, 14, 16], [1, 3, 5, 13, 14, 19], [1, 2, 3, 13, 19, 21], [0, 3, 6, 7, 13, 19], [1, 3, 4, 13, 14, 21], [3, 7, 13, 14, 16, 17], [1, 3, 10, 13, 20, 21]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJar4K7_ADxQ"
      },
      "outputs": [],
      "source": [
        "# Function to split data by specifying test labels\n",
        "def split_by_labels(data_x, data_y, test_labels):\n",
        "    X_train, X_test, y_train, y_test = [], [], [], []\n",
        "    for sample_x, sample_y in zip(data_x, data_y):\n",
        "        if sample_y in test_labels:\n",
        "            X_test.append(sample_x)\n",
        "            y_test.append(sample_y)\n",
        "        else:\n",
        "            X_train.append(sample_x)\n",
        "            y_train.append(sample_y)\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwMU3IQgAq5-"
      },
      "outputs": [],
      "source": [
        "epochs = 80\n",
        "print('Performing one-shot validation for the Angle Usefulness Transformer ...')\n",
        "for test in tests:\n",
        "    print(f'Testing on Exercises: {test} ...')\n",
        "    X_train, X_test, y_train, y_test = split_by_labels(\n",
        "        processed_sequences,\n",
        "        numerical_labels,\n",
        "        test_labels=test,\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    y_train = np.array(y_train, dtype=np.int32)\n",
        "    y_test = np.array(y_test, dtype=np.int32)\n",
        "    y_val = np.array(y_val, dtype=np.int32)\n",
        "\n",
        "    Y_train = np.empty((len(X_train), len(X_train[0][0])), np.float32)\n",
        "    Y_val = np.empty((len(X_val), len(X_train[0][0])), np.float32)\n",
        "    Y_test = np.empty((len(X_test), len(X_train[0][0])), np.float32)\n",
        "\n",
        "    for (Y, y) in [(Y_train, y_train), (Y_val, y_val), (Y_test, y_test)]:\n",
        "        for i in range(Y.shape[0]):\n",
        "            for angle in range(Y.shape[1]):\n",
        "                if angle in label_angles[y[i]]:\n",
        "                    Y[i, angle] = 1\n",
        "                else:\n",
        "                    Y[i, angle] = 0\n",
        "\n",
        "    usefulness_model = angle_usefulness_model(maxlen=MAXLEN, module='transformer', lr=1e-4)\n",
        "\n",
        "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        MODEL_SAVE_PATH_USEFULNESS,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode=\"max\",\n",
        "        save_freq=\"epoch\",\n",
        "        initial_value_threshold=None,\n",
        "    )\n",
        "\n",
        "    pbar_callback = TQDMProgressBar(epochs=epochs)\n",
        "\n",
        "    train_ds = UsefulnessDataset(X_train, Y_train, y_train, batch_size=batch_size, maxlen=MAXLEN, train=True)\n",
        "    val_ds = UsefulnessDataset(X_val, Y_val, y_val, batch_size=min(batch_size, len(X_val)), maxlen=MAXLEN, train=False)\n",
        "\n",
        "    history = usefulness_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[callback, pbar_callback],\n",
        "        verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "    )\n",
        "\n",
        "    # Load the trained classifier model\n",
        "    usefulness_model = tf.keras.models.load_model(\n",
        "        MODEL_SAVE_PATH_USEFULNESS\n",
        "    )\n",
        "\n",
        "    test_ds = UsefulnessDataset(X_test, Y_test, y_test, batch_size=batch_size, maxlen=MAXLEN, train=False)\n",
        "    usefulness_model.evaluate(test_ds, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFhxjwVJARTw"
      },
      "outputs": [],
      "source": [
        "epochs = 60\n",
        "print('Performing one-shot validation for the Siamese Transformer ...')\n",
        "for test in tests:\n",
        "    print(f'Testing on Exercises: {test} ...')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = split_by_labels(\n",
        "        processed_sequences,\n",
        "        numerical_labels,\n",
        "        test_labels=test,\n",
        "    )\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_batches = 10 * len(X_train) // batch_size\n",
        "    val_batches = 10 * len(X_val) // batch_size\n",
        "    train_ds = SimilarityDataset(X_train, y_train, num_batches=train_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=True)\n",
        "    val_ds = SimilarityDataset(X_val, y_val, num_batches=val_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "\n",
        "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        MODEL_SAVE_PATH_SIMILARITY,\n",
        "        monitor=\"val_binary_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        mode=\"max\",\n",
        "        save_freq=\"epoch\",\n",
        "        initial_value_threshold=None,\n",
        "    )\n",
        "\n",
        "    pbar_callback = TQDMProgressBar(epochs=epochs)\n",
        "\n",
        "    # Build and train Siamese model\n",
        "    similarity_model = angle_similarity_model(maxlen=MAXLEN, module='transformer', lr=1e-3)\n",
        "    history = similarity_model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=epochs,\n",
        "        callbacks=[callback, pbar_callback],\n",
        "        verbose=0,  # Change this to 1 if you want progressive display while training\n",
        "    )\n",
        "\n",
        "    # Load the trained Siamese model\n",
        "    similarity_model = tf.keras.models.load_model(\n",
        "        MODEL_SAVE_PATH_SIMILARITY,\n",
        "    )\n",
        "\n",
        "    # Make a large dataset in terms of pair in order to have a more reliable test\n",
        "    # since we can't test on all possible pairs (very huge number)\n",
        "    test_batches = 1000 * len(X_test) // batch_size\n",
        "    # Evaluate the Siamese model on the test set\n",
        "    test_ds = SimilarityDataset(X_test, y_test, num_batches=test_batches, batch_size=batch_size, label_angles=label_angles, maxlen=MAXLEN, train=False)\n",
        "    similarity_model.evaluate(test_ds, verbose=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}